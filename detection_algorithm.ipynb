{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef67e257-571b-4233-9427-ec6f6dad65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b6a4b9-02b2-4089-9955-f195a6db4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolo11l.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b067a7-58a9-445e-a3bf-c3277eef64e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'person',\n",
       " 1: 'bicycle',\n",
       " 2: 'car',\n",
       " 3: 'motorcycle',\n",
       " 4: 'airplane',\n",
       " 5: 'bus',\n",
       " 6: 'train',\n",
       " 7: 'truck',\n",
       " 8: 'boat',\n",
       " 9: 'traffic light',\n",
       " 10: 'fire hydrant',\n",
       " 11: 'stop sign',\n",
       " 12: 'parking meter',\n",
       " 13: 'bench',\n",
       " 14: 'bird',\n",
       " 15: 'cat',\n",
       " 16: 'dog',\n",
       " 17: 'horse',\n",
       " 18: 'sheep',\n",
       " 19: 'cow',\n",
       " 20: 'elephant',\n",
       " 21: 'bear',\n",
       " 22: 'zebra',\n",
       " 23: 'giraffe',\n",
       " 24: 'backpack',\n",
       " 25: 'umbrella',\n",
       " 26: 'handbag',\n",
       " 27: 'tie',\n",
       " 28: 'suitcase',\n",
       " 29: 'frisbee',\n",
       " 30: 'skis',\n",
       " 31: 'snowboard',\n",
       " 32: 'sports ball',\n",
       " 33: 'kite',\n",
       " 34: 'baseball bat',\n",
       " 35: 'baseball glove',\n",
       " 36: 'skateboard',\n",
       " 37: 'surfboard',\n",
       " 38: 'tennis racket',\n",
       " 39: 'bottle',\n",
       " 40: 'wine glass',\n",
       " 41: 'cup',\n",
       " 42: 'fork',\n",
       " 43: 'knife',\n",
       " 44: 'spoon',\n",
       " 45: 'bowl',\n",
       " 46: 'banana',\n",
       " 47: 'apple',\n",
       " 48: 'sandwich',\n",
       " 49: 'orange',\n",
       " 50: 'broccoli',\n",
       " 51: 'carrot',\n",
       " 52: 'hot dog',\n",
       " 53: 'pizza',\n",
       " 54: 'donut',\n",
       " 55: 'cake',\n",
       " 56: 'chair',\n",
       " 57: 'couch',\n",
       " 58: 'potted plant',\n",
       " 59: 'bed',\n",
       " 60: 'dining table',\n",
       " 61: 'toilet',\n",
       " 62: 'tv',\n",
       " 63: 'laptop',\n",
       " 64: 'mouse',\n",
       " 65: 'remote',\n",
       " 66: 'keyboard',\n",
       " 67: 'cell phone',\n",
       " 68: 'microwave',\n",
       " 69: 'oven',\n",
       " 70: 'toaster',\n",
       " 71: 'sink',\n",
       " 72: 'refrigerator',\n",
       " 73: 'book',\n",
       " 74: 'clock',\n",
       " 75: 'vase',\n",
       " 76: 'scissors',\n",
       " 77: 'teddy bear',\n",
       " 78: 'hair drier',\n",
       " 79: 'toothbrush'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = model.names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4765017-c28b-4fdd-8fa1-f3b96d9e50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(r'C:\\Users\\DELL\\Downloads\\853889-hd_1920_1080_25fps.mp4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e117f0-489d-451e-bc1b-0369a3591901",
   "metadata": {},
   "source": [
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO tracking on the frame\n",
    "    results = model.track(frame, persist=True) \n",
    "    print(results)\n",
    "\n",
    "    # Ensure results are not empty\n",
    "    if results[0].boxes.data is not None:\n",
    "        # Get the detected boxes, their class indices, and track IDs\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        if results[0].boxes.id is not None:\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        else:\n",
    "            track_ids = []  # Handle as needed\n",
    "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
    "        confidences = results[0].boxes.conf.cpu()\n",
    "\n",
    "        # Loop through each detected object\n",
    "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cx = (x1 + x2) // 2  \n",
    "            cy = (y1 + y2) // 2            \n",
    "\n",
    "            class_name = class_names[class_idx]\n",
    "\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)           \n",
    "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2) \n",
    "            \n",
    "        cv2.imshow('yolo_tracking', frame)\n",
    "    \n",
    "        if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea5676b4-13f2-4f2a-b327-c73923f9faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store center points for each track_id\n",
    "track_history = {}\n",
    "\n",
    "# Set the maximum number of frames to track\n",
    "max_frames = 20\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO tracking on the frame\n",
    "    results = model.track(frame, persist=True , classes = [0] )\n",
    "    \n",
    "    if results[0].boxes.data is not None:\n",
    "        # Get detected boxes, class indices, and track IDs\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        if results[0].boxes.id is not None:\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        else:\n",
    "            track_ids = []  # Handle as needed\n",
    "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
    "        confidences = results[0].boxes.conf.cpu()\n",
    "\n",
    "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            class_name = class_names[class_idx]\n",
    "\n",
    "            # Update track history\n",
    "            if track_id not in track_history:\n",
    "                track_history[track_id] = []\n",
    "            track_history[track_id].append((cx, cy))\n",
    "            \n",
    "            # Keep only the last 20 frames\n",
    "            if len(track_history[track_id]) > max_frames:\n",
    "                track_history[track_id].pop(0)\n",
    "            \n",
    "            # Draw center point\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Draw the track history\n",
    "            if len(track_history[track_id]) > 1:\n",
    "                for i in range(1, len(track_history[track_id])):\n",
    "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
    "\n",
    "        cv2.imshow('yolo_tracking', frame)\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25202dbd-6c99-4b54-97cc-e7f9dd371fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 26 persons, 2214.7ms\n",
      "Speed: 13.1ms preprocess, 2214.7ms inference, 14.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1490.2ms\n",
      "Speed: 13.0ms preprocess, 1490.2ms inference, 16.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1746.8ms\n",
      "Speed: 8.0ms preprocess, 1746.8ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1870.1ms\n",
      "Speed: 57.2ms preprocess, 1870.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1665.5ms\n",
      "Speed: 13.0ms preprocess, 1665.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1461.3ms\n",
      "Speed: 5.4ms preprocess, 1461.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1400.7ms\n",
      "Speed: 7.0ms preprocess, 1400.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1384.0ms\n",
      "Speed: 6.0ms preprocess, 1384.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1370.4ms\n",
      "Speed: 7.9ms preprocess, 1370.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 3 handbags, 1400.3ms\n",
      "Speed: 3.0ms preprocess, 1400.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 3 handbags, 1365.0ms\n",
      "Speed: 4.4ms preprocess, 1365.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 3 handbags, 1357.6ms\n",
      "Speed: 7.4ms preprocess, 1357.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 3 handbags, 1359.2ms\n",
      "Speed: 6.4ms preprocess, 1359.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1359.6ms\n",
      "Speed: 13.2ms preprocess, 1359.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1369.7ms\n",
      "Speed: 5.9ms preprocess, 1369.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1352.3ms\n",
      "Speed: 2.9ms preprocess, 1352.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1381.2ms\n",
      "Speed: 1.0ms preprocess, 1381.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1413.3ms\n",
      "Speed: 4.7ms preprocess, 1413.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1357.3ms\n",
      "Speed: 3.0ms preprocess, 1357.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1354.6ms\n",
      "Speed: 7.3ms preprocess, 1354.6ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1496.8ms\n",
      "Speed: 3.1ms preprocess, 1496.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1394.0ms\n",
      "Speed: 0.0ms preprocess, 1394.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1360.2ms\n",
      "Speed: 6.8ms preprocess, 1360.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1364.6ms\n",
      "Speed: 7.2ms preprocess, 1364.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1463.8ms\n",
      "Speed: 3.1ms preprocess, 1463.8ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 handbag, 1536.2ms\n",
      "Speed: 9.2ms preprocess, 1536.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1481.0ms\n",
      "Speed: 4.7ms preprocess, 1481.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 handbag, 1348.4ms\n",
      "Speed: 3.2ms preprocess, 1348.4ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1372.8ms\n",
      "Speed: 3.2ms preprocess, 1372.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1338.6ms\n",
      "Speed: 3.3ms preprocess, 1338.6ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1353.1ms\n",
      "Speed: 7.6ms preprocess, 1353.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1386.8ms\n",
      "Speed: 7.5ms preprocess, 1386.8ms inference, 13.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1350.3ms\n",
      "Speed: 3.6ms preprocess, 1350.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store center points for each track_id\n",
    "track_history = {}\n",
    "\n",
    "# Set the maximum number of frames to track\n",
    "max_frames = 20\n",
    "\n",
    "# Counters for total objects and different objects\n",
    "total_objects = 0\n",
    "object_type_counts = {}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO tracking on the frame\n",
    "    results = model.track(frame, persist=True)  # Specify classes if needed\n",
    "    \n",
    "    if results[0].boxes.data is not None:\n",
    "        # Get detected boxes, class indices, and track IDs\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        if results[0].boxes.id is not None:\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        else:\n",
    "            track_ids = []  # Handle as needed\n",
    "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
    "        confidences = results[0].boxes.conf.cpu()\n",
    "\n",
    "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            class_name = class_names[class_idx]\n",
    "\n",
    "            # Update total object count\n",
    "            total_objects += 1\n",
    "\n",
    "            # Update count for this class type\n",
    "            if class_name in object_type_counts:\n",
    "                object_type_counts[class_name] += 1\n",
    "            else:\n",
    "                object_type_counts[class_name] = 1\n",
    "\n",
    "            # Update track history\n",
    "            if track_id not in track_history:\n",
    "                track_history[track_id] = []\n",
    "            track_history[track_id].append((cx, cy))\n",
    "            \n",
    "            # Keep only the last 20 frames\n",
    "            if len(track_history[track_id]) > max_frames:\n",
    "                track_history[track_id].pop(0)\n",
    "            \n",
    "            # Draw center point\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Draw the track history\n",
    "            if len(track_history[track_id]) > 1:\n",
    "                for i in range(1, len(track_history[track_id])):\n",
    "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
    "\n",
    "        # Display counts on the frame\n",
    "        cv2.putText(frame, f\"Total Objects: {total_objects}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        y_offset = 50\n",
    "        for class_name, count in object_type_counts.items():\n",
    "            cv2.putText(frame, f\"{class_name}: {count}\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "            y_offset += 20\n",
    "\n",
    "        cv2.imshow('yolo_tracking', frame)\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06547829-5369-41a9-8977-70c025df4678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 26 persons, 2086.0ms\n",
      "Speed: 12.6ms preprocess, 2086.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1461.3ms\n",
      "Speed: 11.6ms preprocess, 1461.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1345.1ms\n",
      "Speed: 1.0ms preprocess, 1345.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1365.8ms\n",
      "Speed: 5.6ms preprocess, 1365.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1356.8ms\n",
      "Speed: 6.6ms preprocess, 1356.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1391.7ms\n",
      "Speed: 5.5ms preprocess, 1391.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1350.1ms\n",
      "Speed: 7.3ms preprocess, 1350.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1356.5ms\n",
      "Speed: 6.6ms preprocess, 1356.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1381.0ms\n",
      "Speed: 14.0ms preprocess, 1381.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 3 handbags, 1362.9ms\n",
      "Speed: 5.9ms preprocess, 1362.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 3 handbags, 1367.0ms\n",
      "Speed: 3.1ms preprocess, 1367.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 3 handbags, 1352.2ms\n",
      "Speed: 6.6ms preprocess, 1352.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 3 handbags, 1364.7ms\n",
      "Speed: 6.6ms preprocess, 1364.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1463.3ms\n",
      "Speed: 3.2ms preprocess, 1463.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1358.2ms\n",
      "Speed: 5.4ms preprocess, 1358.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1374.2ms\n",
      "Speed: 14.0ms preprocess, 1374.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1360.8ms\n",
      "Speed: 3.1ms preprocess, 1360.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1511.2ms\n",
      "Speed: 6.4ms preprocess, 1511.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1364.9ms\n",
      "Speed: 3.0ms preprocess, 1364.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1353.7ms\n",
      "Speed: 3.1ms preprocess, 1353.7ms inference, 16.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1368.6ms\n",
      "Speed: 2.8ms preprocess, 1368.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1534.1ms\n",
      "Speed: 7.3ms preprocess, 1534.1ms inference, 16.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1367.8ms\n",
      "Speed: 13.8ms preprocess, 1367.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1366.3ms\n",
      "Speed: 6.2ms preprocess, 1366.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1366.0ms\n",
      "Speed: 4.2ms preprocess, 1366.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 handbag, 1358.1ms\n",
      "Speed: 3.2ms preprocess, 1358.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1361.4ms\n",
      "Speed: 1.0ms preprocess, 1361.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 handbag, 1361.4ms\n",
      "Speed: 3.2ms preprocess, 1361.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1355.3ms\n",
      "Speed: 6.6ms preprocess, 1355.3ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1351.4ms\n",
      "Speed: 3.0ms preprocess, 1351.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1363.2ms\n",
      "Speed: 3.3ms preprocess, 1363.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1360.3ms\n",
      "Speed: 1.0ms preprocess, 1360.3ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1357.1ms\n",
      "Speed: 6.4ms preprocess, 1357.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1401.1ms\n",
      "Speed: 3.8ms preprocess, 1401.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1381.8ms\n",
      "Speed: 1.0ms preprocess, 1381.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1370.7ms\n",
      "Speed: 1.0ms preprocess, 1370.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1353.6ms\n",
      "Speed: 3.1ms preprocess, 1353.6ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1339.9ms\n",
      "Speed: 7.2ms preprocess, 1339.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1345.0ms\n",
      "Speed: 18.2ms preprocess, 1345.0ms inference, 16.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1374.1ms\n",
      "Speed: 3.0ms preprocess, 1374.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store center points for each track_id\n",
    "track_history = {}\n",
    "\n",
    "# Set the maximum number of frames to track\n",
    "max_frames = 20\n",
    "\n",
    "# Set to store unique track IDs\n",
    "unique_track_ids = set()\n",
    "\n",
    "# Dictionary to count different objects\n",
    "object_type_counts = {}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO tracking on the frame\n",
    "    results = model.track(frame, persist=True)  # Specify classes if needed\n",
    "    \n",
    "    if results[0].boxes.data is not None:\n",
    "        # Get detected boxes, class indices, and track IDs\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        if results[0].boxes.id is not None:\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        else:\n",
    "            track_ids = []  # Handle as needed\n",
    "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
    "        confidences = results[0].boxes.conf.cpu()\n",
    "\n",
    "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            class_name = class_names[class_idx]\n",
    "\n",
    "            # Update unique track IDs\n",
    "            if track_id not in unique_track_ids:\n",
    "                unique_track_ids.add(track_id)\n",
    "\n",
    "                # Update count for this class type\n",
    "                if class_name in object_type_counts:\n",
    "                    object_type_counts[class_name] += 1\n",
    "                else:\n",
    "                    object_type_counts[class_name] = 1\n",
    "\n",
    "            # Update track history\n",
    "            if track_id not in track_history:\n",
    "                track_history[track_id] = []\n",
    "            track_history[track_id].append((cx, cy))\n",
    "            \n",
    "            # Keep only the last 20 frames\n",
    "            if len(track_history[track_id]) > max_frames:\n",
    "                track_history[track_id].pop(0)\n",
    "            \n",
    "            # Draw center point\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Draw the track history\n",
    "            if len(track_history[track_id]) > 1:\n",
    "                for i in range(1, len(track_history[track_id])):\n",
    "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
    "\n",
    "        # Display counts on the frame\n",
    "        total_objects = len(unique_track_ids)\n",
    "        cv2.putText(frame, f\"Total Objects: {total_objects}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        y_offset = 50\n",
    "        for class_name, count in object_type_counts.items():\n",
    "            cv2.putText(frame, f\"{class_name}: {count}\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "            y_offset += 20\n",
    "\n",
    "        cv2.imshow('yolo_tracking', frame)\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ba5712f-5aca-4026-bc05-0cb8659b1580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 26 persons, 1844.5ms\n",
      "Speed: 15.4ms preprocess, 1844.5ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1470.7ms\n",
      "Speed: 14.0ms preprocess, 1470.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1343.4ms\n",
      "Speed: 15.6ms preprocess, 1343.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1499.7ms\n",
      "Speed: 0.0ms preprocess, 1499.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1405.9ms\n",
      "Speed: 15.6ms preprocess, 1405.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 3 handbags, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 3 handbags, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 3 handbags, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 3 handbags, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1390.3ms\n",
      "Speed: 15.6ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1405.9ms\n",
      "Speed: 0.0ms preprocess, 1405.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1421.5ms\n",
      "Speed: 15.6ms preprocess, 1421.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1405.9ms\n",
      "Speed: 0.0ms preprocess, 1405.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1562.1ms\n",
      "Speed: 0.0ms preprocess, 1562.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 handbag, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 handbag, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1421.5ms\n",
      "Speed: 15.6ms preprocess, 1421.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1390.3ms\n",
      "Speed: 15.6ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1546.5ms\n",
      "Speed: 15.6ms preprocess, 1546.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1343.4ms\n",
      "Speed: 15.6ms preprocess, 1343.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1 tennis racket, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1 tennis racket, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 handbag, 1 tennis racket, 1374.6ms\n",
      "Speed: 15.7ms preprocess, 1374.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 1405.9ms\n",
      "Speed: 15.6ms preprocess, 1405.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 2 handbags, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 backpack, 3 handbags, 1 tennis racket, 1452.8ms\n",
      "Speed: 15.6ms preprocess, 1452.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 backpack, 3 handbags, 1 tennis racket, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 backpack, 2 handbags, 1 tennis racket, 1484.0ms\n",
      "Speed: 0.0ms preprocess, 1484.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 backpack, 3 handbags, 1 tennis racket, 1405.9ms\n",
      "Speed: 0.0ms preprocess, 1405.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 backpack, 2 handbags, 1 tennis racket, 1484.0ms\n",
      "Speed: 0.0ms preprocess, 1484.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 backpack, 2 handbags, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 backpack, 1 handbag, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 backpack, 1 handbag, 1624.6ms\n",
      "Speed: 15.6ms preprocess, 1624.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 backpack, 1 handbag, 1421.5ms\n",
      "Speed: 0.0ms preprocess, 1421.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 backpack, 1 handbag, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 backpack, 1 handbag, 1 tennis racket, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 backpack, 1 handbag, 1 tennis racket, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 backpack, 1 handbag, 1 tennis racket, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 backpack, 1 handbag, 1 tennis racket, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 backpack, 2 handbags, 1 tennis racket, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 backpack, 2 handbags, 1 tennis racket, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 backpack, 3 handbags, 1 tennis racket, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 2 backpacks, 2 handbags, 1 tennis racket, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 backpack, 3 handbags, 1 tennis racket, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 3 handbags, 1 tennis racket, 1421.5ms\n",
      "Speed: 0.0ms preprocess, 1421.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 3 handbags, 1 tennis racket, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 3 handbags, 1 tennis racket, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 handbag, 1 tennis racket, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 handbag, 1 tennis racket, 1343.4ms\n",
      "Speed: 0.0ms preprocess, 1343.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 handbag, 1 tennis racket, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 handbag, 2 tennis rackets, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 handbag, 2 tennis rackets, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 handbag, 2 tennis rackets, 1562.1ms\n",
      "Speed: 0.0ms preprocess, 1562.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 handbag, 2 tennis rackets, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "track_history = {}\n",
    "\n",
    "# Define the circle parameters (x, y, radius)\n",
    "circle_center = (1021, 164)  # Example circle center\n",
    "circle_radius = 100  # Example radius of the circle\n",
    "\n",
    "# Dictionary to store the time spent in the circle for each track_id\n",
    "time_in_circle = {}\n",
    "\n",
    "# Dictionary to store people who are inside the circle\n",
    "people_in_circle = set()\n",
    "\n",
    "# Set the maximum number of frames to track\n",
    "max_frames = 20\n",
    "\n",
    "# Set to store unique track IDs\n",
    "unique_track_ids = set()\n",
    "\n",
    "# Dictionary to count different objects\n",
    "object_type_counts = {}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO tracking on the frame\n",
    "    results = model.track(frame, persist=True)  # Specify classes if needed\n",
    "    \n",
    "    if results[0].boxes.data is not None:\n",
    "        # Get detected boxes, class indices, and track IDs\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        if results[0].boxes.id is not None:\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        else:\n",
    "            track_ids = []  # Handle as needed\n",
    "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
    "        confidences = results[0].boxes.conf.cpu()\n",
    "\n",
    "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            class_name = class_names[class_idx]\n",
    "\n",
    "            # Update unique track IDs\n",
    "            if track_id not in unique_track_ids:\n",
    "                unique_track_ids.add(track_id)\n",
    "\n",
    "                # Update count for this class type\n",
    "                if class_name in object_type_counts:\n",
    "                    object_type_counts[class_name] += 1\n",
    "                else:\n",
    "                    object_type_counts[class_name] = 1\n",
    "\n",
    "            # Update track history\n",
    "            if track_id not in track_history:\n",
    "                track_history[track_id] = []\n",
    "            track_history[track_id].append((cx, cy))\n",
    "\n",
    "            # Keep only the last 20 frames\n",
    "            if len(track_history[track_id]) > max_frames:\n",
    "                track_history[track_id].pop(0)\n",
    "\n",
    "            # Check if the person is inside the circle\n",
    "            distance_to_center = ((cx - circle_center[0]) ** 2 + (cy - circle_center[1]) ** 2) ** 0.5\n",
    "            if distance_to_center <= circle_radius:\n",
    "                if track_id not in people_in_circle:\n",
    "                    people_in_circle.add(track_id)\n",
    "                    time_in_circle[track_id] = time.time()  # Track the entry time for new people\n",
    "\n",
    "            # If the person is inside the circle, calculate the time spent\n",
    "            if track_id in people_in_circle:\n",
    "                time_spent = time.time() - time_in_circle[track_id]\n",
    "                minutes_spent = time_spent / 60  # Convert seconds to minutes\n",
    "                cv2.putText(frame, f\"Time in Circle: {minutes_spent:.2f} min\", (x1, y1 - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "\n",
    "            # Draw center point and track information\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Draw the track history\n",
    "            if len(track_history[track_id]) > 1:\n",
    "                for i in range(1, len(track_history[track_id])):\n",
    "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
    "\n",
    "        # Display counts on the frame\n",
    "        total_objects = len(unique_track_ids)\n",
    "        cv2.putText(frame, f\"Total Objects: {total_objects}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        y_offset = 50\n",
    "        for class_name, count in object_type_counts.items():\n",
    "            cv2.putText(frame, f\"{class_name}: {count}\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "            y_offset += 20\n",
    "\n",
    "        # Draw the circle on the frame\n",
    "        cv2.circle(frame, circle_center, circle_radius, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imshow('yolo_tracking', frame)\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09752b44-0bdd-42c8-bd63-deb411abac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Initialize variables\n",
    "circle_radius = 100\n",
    "circle_center = None\n",
    "\n",
    "# Mouse callback function to set the center of the circle\n",
    "def set_circle_position(event, x, y, flags, param):\n",
    "    global circle_center\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:  # When left mouse button is clicked\n",
    "        circle_center = (x, y)  # Set the center of the circle to the click position\n",
    "\n",
    "# Create a window and bind the mouse callback function\n",
    "cv2.namedWindow('Select Circle Position')\n",
    "cv2.setMouseCallback('Select Circle Position', set_circle_position)\n",
    "\n",
    "# Start video capture (or any frame source)\n",
    "cap = cv2.VideoCapture(0)  # You can replace this with your video stream source\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    if circle_center:\n",
    "        # Draw the circle on the frame at the selected position\n",
    "        cv2.circle(frame, circle_center, circle_radius, (0, 0, 255), 2)\n",
    "\n",
    "    # Show the frame with circle\n",
    "    cv2.imshow('Select Circle Position', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4698adbb-34f7-41e0-b633-da59985e0260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Circle Center: (1021, 164)\n",
      "Selected Circle Center: (425, 512)\n",
      "Selected Circle Center: (1204, 60)\n",
      "Selected Circle Center: (940, 502)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Initialize variables\n",
    "circle_radius = 100\n",
    "circle_center = None\n",
    "\n",
    "# Mouse callback function to set the center of the circle\n",
    "def set_circle_position(event, x, y, flags, param):\n",
    "    global circle_center\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:  # When left mouse button is clicked\n",
    "        circle_center = (x, y)  # Set the center of the circle to the click position\n",
    "        print(f\"Selected Circle Center: {circle_center}\")  # Print the selected position\n",
    "\n",
    "# Create a window and bind the mouse callback function\n",
    "cv2.namedWindow('Select Circle Position')\n",
    "cv2.setMouseCallback('Select Circle Position', set_circle_position)\n",
    "\n",
    "# Start video capture (use a video file instead of the webcam)\n",
    "video_path = r'C:\\Users\\DELL\\Downloads\\853889-hd_1920_1080_25fps.mp4'  # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    if circle_center:\n",
    "        # Draw the circle on the frame at the selected position\n",
    "        cv2.circle(frame, circle_center, circle_radius, (0, 0, 255), 2)\n",
    "\n",
    "    # Show the frame with circle\n",
    "    cv2.imshow('Select Circle Position', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92a22e-4c88-4a43-883c-721dedad25fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bad52c-0b88-48a8-a3fd-d5502a2714f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156fd68-710a-43ac-9085-3d9099626d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18972d03-42b0-4bb5-8c04-d6b3690f64a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d1026-4884-4b6d-af04-88896f66cd82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a20f7-8387-4b1b-acf4-ef298cbec6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "972a0f7d-08b5-4525-ab3d-d5fd745aa396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store center points for each track_id\n",
    "track_history = {}\n",
    "\n",
    "# Set the maximum number of frames to track\n",
    "max_frames = 20\n",
    "\n",
    "# Set to store unique track IDs\n",
    "unique_track_ids = set()\n",
    "\n",
    "# Dictionary to count different objects\n",
    "object_type_counts = {}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO tracking on the frame\n",
    "    results = model.track(frame, persist=True)  # Specify classes if needed\n",
    "    \n",
    "    if results[0].boxes.data is not None:\n",
    "        # Get detected boxes, class indices, and track IDs\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        if results[0].boxes.id is not None:\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        else:\n",
    "            track_ids = []  # Handle as needed\n",
    "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
    "        confidences = results[0].boxes.conf.cpu()\n",
    "\n",
    "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            class_name = class_names[class_idx]\n",
    "\n",
    "            # Update unique track IDs\n",
    "            if track_id not in unique_track_ids:\n",
    "                unique_track_ids.add(track_id)\n",
    "\n",
    "                # Update count for this class type\n",
    "                if class_name in object_type_counts:\n",
    "                    object_type_counts[class_name] += 1\n",
    "                else:\n",
    "                    object_type_counts[class_name] = 1\n",
    "\n",
    "            # Update track history\n",
    "            if track_id not in track_history:\n",
    "                track_history[track_id] = []\n",
    "            track_history[track_id].append((cx, cy))\n",
    "            \n",
    "            # Keep only the last 20 frames\n",
    "            if len(track_history[track_id]) > max_frames:\n",
    "                track_history[track_id].pop(0)\n",
    "            \n",
    "            # Draw center point\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Draw the track history\n",
    "            if len(track_history[track_id]) > 1:\n",
    "                for i in range(1, len(track_history[track_id])):\n",
    "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
    "\n",
    "        # Display counts on the frame\n",
    "        total_objects = len(unique_track_ids)\n",
    "        cv2.putText(frame, f\"Total Objects: {total_objects}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        y_offset = 50\n",
    "        for class_name, count in object_type_counts.items():\n",
    "            cv2.putText(frame, f\"{class_name}: {count}\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "            y_offset += 20\n",
    "\n",
    "        cv2.imshow('yolo_tracking', frame)\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab50b9e8-e6c3-4b51-896d-bac3a5b39bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 16 persons, 1595.5ms\n",
      "Speed: 6.7ms preprocess, 1595.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1370.6ms\n",
      "Speed: 5.4ms preprocess, 1370.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1348.3ms\n",
      "Speed: 6.8ms preprocess, 1348.3ms inference, 16.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1368.9ms\n",
      "Speed: 6.4ms preprocess, 1368.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1363.5ms\n",
      "Speed: 6.6ms preprocess, 1363.5ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1350.6ms\n",
      "Speed: 5.9ms preprocess, 1350.6ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1366.0ms\n",
      "Speed: 5.9ms preprocess, 1366.0ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1366.9ms\n",
      "Speed: 5.8ms preprocess, 1366.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 handbag, 1350.7ms\n",
      "Speed: 8.2ms preprocess, 1350.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 3 handbags, 1339.8ms\n",
      "Speed: 16.6ms preprocess, 1339.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 3 handbags, 1355.2ms\n",
      "Speed: 5.2ms preprocess, 1355.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 handbags, 1348.2ms\n",
      "Speed: 8.0ms preprocess, 1348.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 3 handbags, 1373.1ms\n",
      "Speed: 2.7ms preprocess, 1373.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 handbags, 1355.9ms\n",
      "Speed: 6.5ms preprocess, 1355.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1373.1ms\n",
      "Speed: 4.5ms preprocess, 1373.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1340.9ms\n",
      "Speed: 6.2ms preprocess, 1340.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1409.1ms\n",
      "Speed: 17.2ms preprocess, 1409.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1344.8ms\n",
      "Speed: 5.1ms preprocess, 1344.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1353.5ms\n",
      "Speed: 4.3ms preprocess, 1353.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1348.8ms\n",
      "Speed: 4.6ms preprocess, 1348.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1566.9ms\n",
      "Speed: 0.0ms preprocess, 1566.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1364.3ms\n",
      "Speed: 7.7ms preprocess, 1364.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 handbag, 1362.7ms\n",
      "Speed: 2.9ms preprocess, 1362.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 handbag, 1364.1ms\n",
      "Speed: 5.7ms preprocess, 1364.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 handbag, 1356.4ms\n",
      "Speed: 5.3ms preprocess, 1356.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 handbag, 1345.2ms\n",
      "Speed: 5.9ms preprocess, 1345.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1353.0ms\n",
      "Speed: 3.1ms preprocess, 1353.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 1360.3ms\n",
      "Speed: 3.4ms preprocess, 1360.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1364.6ms\n",
      "Speed: 2.0ms preprocess, 1364.6ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 handbags, 1360.3ms\n",
      "Speed: 4.0ms preprocess, 1360.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 handbags, 1 tennis racket, 1361.8ms\n",
      "Speed: 5.4ms preprocess, 1361.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1358.4ms\n",
      "Speed: 1.8ms preprocess, 1358.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1337.9ms\n",
      "Speed: 4.9ms preprocess, 1337.9ms inference, 17.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1364.3ms\n",
      "Speed: 0.0ms preprocess, 1364.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1360.2ms\n",
      "Speed: 3.6ms preprocess, 1360.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1365.0ms\n",
      "Speed: 3.0ms preprocess, 1365.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1394.7ms\n",
      "Speed: 4.2ms preprocess, 1394.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1366.2ms\n",
      "Speed: 4.1ms preprocess, 1366.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1362.0ms\n",
      "Speed: 1.6ms preprocess, 1362.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1384.8ms\n",
      "Speed: 0.0ms preprocess, 1384.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1542.3ms\n",
      "Speed: 0.0ms preprocess, 1542.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1360.8ms\n",
      "Speed: 6.7ms preprocess, 1360.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1352.7ms\n",
      "Speed: 7.6ms preprocess, 1352.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1348.1ms\n",
      "Speed: 3.2ms preprocess, 1348.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 handbag, 1338.4ms\n",
      "Speed: 18.9ms preprocess, 1338.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 handbag, 1345.8ms\n",
      "Speed: 17.4ms preprocess, 1345.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1353.1ms\n",
      "Speed: 1.0ms preprocess, 1353.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1369.4ms\n",
      "Speed: 1.0ms preprocess, 1369.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1340.8ms\n",
      "Speed: 4.3ms preprocess, 1340.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 handbag, 1340.8ms\n",
      "Speed: 14.9ms preprocess, 1340.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 handbag, 1397.4ms\n",
      "Speed: 7.2ms preprocess, 1397.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1348.8ms\n",
      "Speed: 16.7ms preprocess, 1348.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 2 handbags, 1358.3ms\n",
      "Speed: 3.1ms preprocess, 1358.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 backpack, 1 handbag, 1351.2ms\n",
      "Speed: 4.1ms preprocess, 1351.2ms inference, 15.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 handbag, 1349.0ms\n",
      "Speed: 6.8ms preprocess, 1349.0ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1358.7ms\n",
      "Speed: 5.2ms preprocess, 1358.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1355.8ms\n",
      "Speed: 3.2ms preprocess, 1355.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1357.7ms\n",
      "Speed: 3.5ms preprocess, 1357.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1353.6ms\n",
      "Speed: 3.2ms preprocess, 1353.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1355.1ms\n",
      "Speed: 2.6ms preprocess, 1355.1ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1552.5ms\n",
      "Speed: 7.4ms preprocess, 1552.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 handbag, 1399.0ms\n",
      "Speed: 2.9ms preprocess, 1399.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 handbag, 1346.7ms\n",
      "Speed: 6.4ms preprocess, 1346.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 handbag, 1 tennis racket, 1345.2ms\n",
      "Speed: 3.1ms preprocess, 1345.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1341.8ms\n",
      "Speed: 3.1ms preprocess, 1341.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1349.7ms\n",
      "Speed: 3.3ms preprocess, 1349.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1356.5ms\n",
      "Speed: 3.8ms preprocess, 1356.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 3 handbags, 1 tennis racket, 1393.2ms\n",
      "Speed: 3.3ms preprocess, 1393.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1354.5ms\n",
      "Speed: 3.1ms preprocess, 1354.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 backpack, 3 handbags, 1 tennis racket, 1352.4ms\n",
      "Speed: 3.1ms preprocess, 1352.4ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 backpack, 3 handbags, 1 tennis racket, 1387.4ms\n",
      "Speed: 2.9ms preprocess, 1387.4ms inference, 17.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Initial confidence threshold\n",
    "confidence_threshold = 50  # Percentage (0-100)\n",
    "\n",
    "# Callback function for the trackbar\n",
    "def update_confidence(x):\n",
    "    global confidence_threshold\n",
    "    confidence_threshold = x / 100  # Convert to decimal (0.0-1.0)\n",
    "\n",
    "# Create a window\n",
    "cv2.namedWindow('yolo_tracking')\n",
    "\n",
    "# Create the trackbar\n",
    "cv2.createTrackbar('Confidence', 'yolo_tracking', 50, 100, update_confidence)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO tracking with the current confidence threshold\n",
    "    results = model.track(frame, persist=True, conf=confidence_threshold)\n",
    "\n",
    "    if results[0].boxes.data is not None:\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        if results[0].boxes.id is not None:\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        else:\n",
    "            track_ids = []\n",
    "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
    "        confidences = results[0].boxes.conf.cpu()\n",
    "\n",
    "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            class_name = class_names[class_idx]\n",
    "\n",
    "            # Draw center point\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the current confidence threshold\n",
    "    cv2.putText(frame, f\"Confidence: {confidence_threshold:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('yolo_tracking', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6667b0a6-03c8-4eda-917d-6ba13bd46249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 16 persons, 1598.6ms\n",
      "Speed: 6.8ms preprocess, 1598.6ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1352.7ms\n",
      "Speed: 3.3ms preprocess, 1352.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1357.5ms\n",
      "Speed: 3.1ms preprocess, 1357.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1344.2ms\n",
      "Speed: 6.3ms preprocess, 1344.2ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1358.1ms\n",
      "Speed: 6.5ms preprocess, 1358.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1344.2ms\n",
      "Speed: 5.5ms preprocess, 1344.2ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 handbag, 1373.9ms\n",
      "Speed: 7.9ms preprocess, 1373.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1344.2ms\n",
      "Speed: 6.6ms preprocess, 1344.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1355.5ms\n",
      "Speed: 16.5ms preprocess, 1355.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1372.2ms\n",
      "Speed: 10.0ms preprocess, 1372.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1354.7ms\n",
      "Speed: 7.5ms preprocess, 1354.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1351.6ms\n",
      "Speed: 3.0ms preprocess, 1351.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1334.1ms\n",
      "Speed: 5.6ms preprocess, 1334.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1351.2ms\n",
      "Speed: 5.9ms preprocess, 1351.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1359.5ms\n",
      "Speed: 1.0ms preprocess, 1359.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1338.7ms\n",
      "Speed: 12.8ms preprocess, 1338.7ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1337.4ms\n",
      "Speed: 8.6ms preprocess, 1337.4ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1362.7ms\n",
      "Speed: 6.2ms preprocess, 1362.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 handbag, 1361.3ms\n",
      "Speed: 3.3ms preprocess, 1361.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1342.8ms\n",
      "Speed: 4.3ms preprocess, 1342.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1591.6ms\n",
      "Speed: 2.9ms preprocess, 1591.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1541.1ms\n",
      "Speed: 2.9ms preprocess, 1541.1ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1412.8ms\n",
      "Speed: 12.2ms preprocess, 1412.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1352.0ms\n",
      "Speed: 1.0ms preprocess, 1352.0ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1366.0ms\n",
      "Speed: 2.8ms preprocess, 1366.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1358.1ms\n",
      "Speed: 11.6ms preprocess, 1358.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 handbag, 1360.8ms\n",
      "Speed: 2.7ms preprocess, 1360.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 handbag, 1351.2ms\n",
      "Speed: 16.4ms preprocess, 1351.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 handbag, 1350.6ms\n",
      "Speed: 3.0ms preprocess, 1350.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1358.7ms\n",
      "Speed: 1.0ms preprocess, 1358.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 handbags, 1371.1ms\n",
      "Speed: 1.0ms preprocess, 1371.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1346.8ms\n",
      "Speed: 3.3ms preprocess, 1346.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 tennis racket, 1359.3ms\n",
      "Speed: 3.3ms preprocess, 1359.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 handbags, 1 tennis racket, 1341.7ms\n",
      "Speed: 3.1ms preprocess, 1341.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 handbags, 1 tennis racket, 1352.4ms\n",
      "Speed: 3.1ms preprocess, 1352.4ms inference, 14.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1370.5ms\n",
      "Speed: 2.2ms preprocess, 1370.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1372.4ms\n",
      "Speed: 1.0ms preprocess, 1372.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1328.7ms\n",
      "Speed: 5.5ms preprocess, 1328.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1355.9ms\n",
      "Speed: 3.0ms preprocess, 1355.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1427.0ms\n",
      "Speed: 6.4ms preprocess, 1427.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1559.4ms\n",
      "Speed: 3.6ms preprocess, 1559.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1352.3ms\n",
      "Speed: 9.4ms preprocess, 1352.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1358.6ms\n",
      "Speed: 3.2ms preprocess, 1358.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1338.6ms\n",
      "Speed: 3.1ms preprocess, 1338.6ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 handbag, 1370.8ms\n",
      "Speed: 4.1ms preprocess, 1370.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 handbag, 1355.7ms\n",
      "Speed: 3.0ms preprocess, 1355.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1413.6ms\n",
      "Speed: 7.2ms preprocess, 1413.6ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1350.9ms\n",
      "Speed: 6.7ms preprocess, 1350.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1489.1ms\n",
      "Speed: 3.0ms preprocess, 1489.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1359.5ms\n",
      "Speed: 3.1ms preprocess, 1359.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1364.1ms\n",
      "Speed: 1.0ms preprocess, 1364.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 1343.9ms\n",
      "Speed: 4.9ms preprocess, 1343.9ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Initial confidence and NMS thresholds (in percentage)\n",
    "confidence_threshold = 50  # Confidence threshold in percentage (0-100)\n",
    "nms_threshold = 40  # NMS threshold in percentage (0-100)\n",
    "\n",
    "# Callback function for the confidence trackbar\n",
    "def update_confidence(x):\n",
    "    global confidence_threshold\n",
    "    confidence_threshold = x / 100  # Convert to decimal (0.0-1.0)\n",
    "\n",
    "# Callback function for the NMS trackbar\n",
    "def update_nms(x):\n",
    "    global nms_threshold\n",
    "    nms_threshold = x / 100  # Convert to decimal (0.0-1.0)\n",
    "\n",
    "# Create a window for displaying the video and parameters\n",
    "cv2.namedWindow('yolo_tracking')\n",
    "\n",
    "# Create trackbars for confidence and NMS thresholds\n",
    "cv2.createTrackbar('Confidence', 'yolo_tracking', 50, 100, update_confidence)\n",
    "cv2.createTrackbar('NMS Threshold', 'yolo_tracking', 40, 100, update_nms)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO tracking with current confidence and NMS thresholds\n",
    "    results = model.track(frame, persist=True, conf=confidence_threshold)\n",
    "\n",
    "    if results[0].boxes.data is not None:\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        if results[0].boxes.id is not None:\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        else:\n",
    "            track_ids = []\n",
    "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
    "        confidences = results[0].boxes.conf.cpu()\n",
    "\n",
    "        # Apply NMS with the current NMS threshold\n",
    "        indices = cv2.dnn.NMSBoxes(boxes.tolist(), confidences.tolist(), score_threshold=confidence_threshold, nms_threshold=nms_threshold)\n",
    "\n",
    "        for i in indices.flatten():\n",
    "            x1, y1, x2, y2 = map(int, boxes[i])\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            class_name = class_names[class_indices[i]]\n",
    "\n",
    "            # Draw center point\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"ID: {track_ids[i]} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the current confidence and NMS threshold values\n",
    "    cv2.putText(frame, f\"Confidence: {confidence_threshold:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"NMS Threshold: {nms_threshold:.2f}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame with the bounding boxes and tracking\n",
    "    cv2.imshow('yolo_tracking', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860fa7ec-9ff0-41aa-b1e1-d64f265e0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Initial confidence and NMS thresholds (in percentage)\n",
    "confidence_threshold = 50  # Confidence threshold in percentage (0-100)\n",
    "nms_threshold = 40  # NMS threshold in percentage (0-100)\n",
    "\n",
    "# Callback function for the confidence trackbar\n",
    "def update_confidence(x):\n",
    "    global confidence_threshold\n",
    "    confidence_threshold = x / 100  # Convert to decimal (0.0-1.0)\n",
    "\n",
    "# Callback function for the NMS trackbar\n",
    "def update_nms(x):\n",
    "    global nms_threshold\n",
    "    nms_threshold = x / 100  # Convert to decimal (0.0-1.0)\n",
    "\n",
    "# Create a window for displaying the video and parameters\n",
    "cv2.namedWindow('yolo_tracking')\n",
    "\n",
    "# Create trackbars for confidence and NMS thresholds\n",
    "cv2.createTrackbar('Confidence', 'yolo_tracking', 50, 100, update_confidence)\n",
    "cv2.createTrackbar('NMS Threshold', 'yolo_tracking', 40, 100, update_nms)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO tracking with current confidence and NMS thresholds\n",
    "    results = model.track(frame, persist=True, conf=confidence_threshold)\n",
    "\n",
    "    if results[0].boxes.data is not None:\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        if results[0].boxes.id is not None:\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        else:\n",
    "            track_ids = []\n",
    "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
    "        confidences = results[0].boxes.conf.cpu()\n",
    "\n",
    "        # Apply NMS with the current NMS threshold\n",
    "        indices = cv2.dnn.NMSBoxes(boxes.tolist(), confidences.tolist(), score_threshold=confidence_threshold, nms_threshold=nms_threshold)\n",
    "\n",
    "        for i in indices.flatten():\n",
    "            x1, y1, x2, y2 = map(int, boxes[i])\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            class_name = class_names[class_indices[i]]\n",
    "\n",
    "            # Draw center point\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"ID: {track_ids[i]} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the current confidence and NMS threshold values\n",
    "    cv2.putText(frame, f\"Confidence: {confidence_threshold:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"NMS Threshold: {nms_threshold:.2f}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame with the bounding boxes and tracking\n",
    "    cv2.imshow('yolo_tracking', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089eab7-6152-44d0-aae3-43f89fea1a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ae2de-e8ec-4f11-8288-a626a78a96c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0c06e-c25f-4a5d-8180-98c73d6a9e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a89b53-9aeb-43a5-9f73-1befd3086c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fecb3f5-49cf-430b-9d9a-62726be86a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b23e2-157a-42a7-9b5f-9fcc1b44a71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf826cf4-36c1-4667-9216-b9a711040a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62d74a-98a6-4303-828c-c93e1b452438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19c611-502a-4cdf-992f-67df82ed6d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc9907-7bd3-4884-a191-55f819ccf0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a15f43-083e-4363-99b6-488f571b6730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca5fbd-fefc-45a8-8eeb-50a1c4c4465b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f1fd917-3fa8-4ad1-8ccd-b03723f9e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store center points for each track_id\n",
    "track_history = {}\n",
    "\n",
    "# Set the maximum number of frames to track\n",
    "max_frames = 20\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO tracking on the frame\n",
    "    results = model.track(frame, persist=True , classes = [0] )\n",
    "    \n",
    "    if results[0].boxes.data is not None:\n",
    "        # Get detected boxes, class indices, and track IDs\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        if results[0].boxes.id is not None:\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        else:\n",
    "            track_ids = []  # Handle as needed\n",
    "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
    "        confidences = results[0].boxes.conf.cpu()\n",
    "\n",
    "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            class_name = class_names[class_idx]\n",
    "\n",
    "            # Update track history\n",
    "            if track_id not in track_history:\n",
    "                track_history[track_id] = []\n",
    "            track_history[track_id].append((cx, cy))\n",
    "            \n",
    "            # Keep only the last 20 frames\n",
    "            if len(track_history[track_id]) > max_frames:\n",
    "                track_history[track_id].pop(0)\n",
    "            \n",
    "            # Draw center point\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Draw the track history\n",
    "            if len(track_history[track_id]) > 1:\n",
    "                for i in range(1, len(track_history[track_id])):\n",
    "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
    "\n",
    "        cv2.imshow('yolo_tracking', frame)\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97d321ca-95c4-4cbe-9084-9ef82a51373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 28 persons, 1768.6ms\n",
      "Speed: 10.0ms preprocess, 1768.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1426.6ms\n",
      "Speed: 15.8ms preprocess, 1426.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Entry detected: ID 20\n",
      "\n",
      "0: 384x640 28 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1405.9ms\n",
      "Speed: 0.0ms preprocess, 1405.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1530.9ms\n",
      "Speed: 15.6ms preprocess, 1530.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1343.4ms\n",
      "Speed: 15.6ms preprocess, 1343.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1343.4ms\n",
      "Speed: 15.6ms preprocess, 1343.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1343.4ms\n",
      "Speed: 15.6ms preprocess, 1343.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1468.4ms\n",
      "Speed: 0.0ms preprocess, 1468.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1499.7ms\n",
      "Speed: 15.6ms preprocess, 1499.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Exit detected: ID 3\n",
      "\n",
      "0: 384x640 26 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1343.4ms\n",
      "Speed: 15.6ms preprocess, 1343.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1546.5ms\n",
      "Speed: 15.6ms preprocess, 1546.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Exit detected: ID 18\n",
      "\n",
      "0: 384x640 26 persons, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.0ms\n",
      "Speed: 0.0ms preprocess, 1359.0ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1546.5ms\n",
      "Speed: 15.6ms preprocess, 1546.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1343.5ms\n",
      "Speed: 0.0ms preprocess, 1343.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1546.5ms\n",
      "Speed: 15.6ms preprocess, 1546.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1359.0ms\n",
      "Speed: 0.0ms preprocess, 1359.0ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1343.4ms\n",
      "Speed: 0.0ms preprocess, 1343.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1343.5ms\n",
      "Speed: 15.6ms preprocess, 1343.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1390.3ms\n",
      "Speed: 15.6ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1374.6ms\n",
      "Speed: 0.0ms preprocess, 1374.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Exit detected: ID 1\n",
      "\n",
      "0: 384x640 30 persons, 1343.4ms\n",
      "Speed: 15.6ms preprocess, 1343.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1374.7ms\n",
      "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1546.5ms\n",
      "Speed: 0.0ms preprocess, 1546.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1390.3ms\n",
      "Speed: 15.6ms preprocess, 1390.3ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1468.4ms\n",
      "Speed: 0.0ms preprocess, 1468.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1405.9ms\n",
      "Speed: 0.0ms preprocess, 1405.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Entry detected: ID 56\n",
      "\n",
      "0: 384x640 30 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1359.0ms\n",
      "Speed: 15.6ms preprocess, 1359.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 31 persons, 1390.3ms\n",
      "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 31 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 31 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 30 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1530.9ms\n",
      "Speed: 0.0ms preprocess, 1530.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1359.1ms\n",
      "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1359.1ms\n",
      "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1405.9ms\n",
      "Speed: 0.0ms preprocess, 1405.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1374.7ms\n",
      "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "max_frames = 30\n",
    "# Define horizontal line positions\n",
    "entry_line_y = 100\n",
    "exit_line_y = 600\n",
    "\n",
    "# Counters for entries and exits\n",
    "entry_count = 0\n",
    "exit_count = 0\n",
    "\n",
    "# Add the following inside your while loop\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Draw horizontal lines\n",
    "    cv2.line(frame, (0, entry_line_y), (frame.shape[1], entry_line_y), (0, 255, 0), 2)  # Entry line\n",
    "    cv2.line(frame, (0, exit_line_y), (frame.shape[1], exit_line_y), (0, 0, 255), 2)    # Exit line\n",
    "\n",
    "    # Run YOLO tracking on the frame\n",
    "    results = model.track(frame, persist=True, classes=[0])\n",
    "\n",
    "    if results[0].boxes.data is not None:\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "        if results[0].boxes.id is not None:\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        else:\n",
    "            track_ids = []\n",
    "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
    "        confidences = results[0].boxes.conf.cpu()\n",
    "\n",
    "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            class_name = class_names[class_idx]\n",
    "\n",
    "            # Update track history\n",
    "            if track_id not in track_history:\n",
    "                track_history[track_id] = []\n",
    "            track_history[track_id].append((cx, cy))\n",
    "\n",
    "            if len(track_history[track_id]) > max_frames:\n",
    "                track_history[track_id].pop(0)\n",
    "\n",
    "            # Check crossing entry and exit lines\n",
    "            if len(track_history[track_id]) > 1:\n",
    "                prev_cy = track_history[track_id][-2][1]\n",
    "                \n",
    "                # Check entry\n",
    "                if prev_cy < entry_line_y <= cy:\n",
    "                    entry_count += 1\n",
    "                    print(f\"Entry detected: ID {track_id}\")\n",
    "\n",
    "                # Check exit\n",
    "                if prev_cy > exit_line_y >= cy:\n",
    "                    exit_count += 1\n",
    "                    print(f\"Exit detected: ID {track_id}\")\n",
    "\n",
    "            # Draw center point and track\n",
    "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Draw track history\n",
    "            if len(track_history[track_id]) > 1:\n",
    "                for i in range(1, len(track_history[track_id])):\n",
    "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
    "\n",
    "    # Display entry/exit counts\n",
    "    cv2.putText(frame, f\"Entries: {entry_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Exits: {exit_count}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow('yolo_tracking', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673d34f-81d0-440d-a893-922640918d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.47 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.38  Python-3.11.5 torch-2.1.0+cpu CPU (Intel Core(TM) i3-6006U 2.00GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11l.pt, data=G:\\number_plate\\data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to 'C:\\Users\\DELL\\AppData\\Roaming\\Ultralytics\\Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 755k/755k [00:00<00:00, 2.34MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   1411795  ultralytics.nn.modules.head.Detect           [1, [256, 512, 512]]          \n",
      "YOLO11l summary: 631 layers, 25,311,251 parameters, 25,311,235 gradients, 87.3 GFLOPs\n",
      "\n",
      "Transferred 174/1015 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning G:\\number_plate\\train\\labels... 22 images, 0 backgrounds, 0 corrupt: 100%|██████████| 22/22 [00:00<00:0\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: G:\\number_plate\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\albumentations\\__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.21). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning G:\\number_plate\\valid\\labels... 13 images, 0 backgrounds, 0 corrupt: 100%|██████████| 13/13 [00:00<00:00,\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: G:\\number_plate\\valid\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 167 weight(decay=0.0), 174 weight(decay=0.0005), 173 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_results = model.train(\n",
    "    data=r\"G:\\number_plate\\data.yaml\",  \n",
    "    epochs=10, \n",
    "    imgsz=640,  \n",
    "    device=\"cpu\",  \n",
    ")\n",
    "\n",
    "# Evaluate model performance on the validation set\n",
    "metrics = model.val()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4604d-2f70-4c6e-8fd8-11775dad43b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Export the model to ONNX format\n",
    "path = model.export(format=\"onnx\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce2ddd-847b-4971-b41a-fb2410e7d31a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907e564-4c18-4484-9b1b-e46f6f399432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
