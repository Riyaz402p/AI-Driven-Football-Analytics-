{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef67e257-571b-4233-9427-ec6f6dad65ef",
      "metadata": {
        "id": "ef67e257-571b-4233-9427-ec6f6dad65ef"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0b6a4b9-02b2-4089-9955-f195a6db4a5b",
      "metadata": {
        "id": "f0b6a4b9-02b2-4089-9955-f195a6db4a5b"
      },
      "outputs": [],
      "source": [
        "model = YOLO('yolo11l.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b067a7-58a9-445e-a3bf-c3277eef64e2",
      "metadata": {
        "id": "51b067a7-58a9-445e-a3bf-c3277eef64e2",
        "outputId": "ffbe7a2b-4ea0-4756-e9f3-db08718dc067"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'person',\n",
              " 1: 'bicycle',\n",
              " 2: 'car',\n",
              " 3: 'motorcycle',\n",
              " 4: 'airplane',\n",
              " 5: 'bus',\n",
              " 6: 'train',\n",
              " 7: 'truck',\n",
              " 8: 'boat',\n",
              " 9: 'traffic light',\n",
              " 10: 'fire hydrant',\n",
              " 11: 'stop sign',\n",
              " 12: 'parking meter',\n",
              " 13: 'bench',\n",
              " 14: 'bird',\n",
              " 15: 'cat',\n",
              " 16: 'dog',\n",
              " 17: 'horse',\n",
              " 18: 'sheep',\n",
              " 19: 'cow',\n",
              " 20: 'elephant',\n",
              " 21: 'bear',\n",
              " 22: 'zebra',\n",
              " 23: 'giraffe',\n",
              " 24: 'backpack',\n",
              " 25: 'umbrella',\n",
              " 26: 'handbag',\n",
              " 27: 'tie',\n",
              " 28: 'suitcase',\n",
              " 29: 'frisbee',\n",
              " 30: 'skis',\n",
              " 31: 'snowboard',\n",
              " 32: 'sports ball',\n",
              " 33: 'kite',\n",
              " 34: 'baseball bat',\n",
              " 35: 'baseball glove',\n",
              " 36: 'skateboard',\n",
              " 37: 'surfboard',\n",
              " 38: 'tennis racket',\n",
              " 39: 'bottle',\n",
              " 40: 'wine glass',\n",
              " 41: 'cup',\n",
              " 42: 'fork',\n",
              " 43: 'knife',\n",
              " 44: 'spoon',\n",
              " 45: 'bowl',\n",
              " 46: 'banana',\n",
              " 47: 'apple',\n",
              " 48: 'sandwich',\n",
              " 49: 'orange',\n",
              " 50: 'broccoli',\n",
              " 51: 'carrot',\n",
              " 52: 'hot dog',\n",
              " 53: 'pizza',\n",
              " 54: 'donut',\n",
              " 55: 'cake',\n",
              " 56: 'chair',\n",
              " 57: 'couch',\n",
              " 58: 'potted plant',\n",
              " 59: 'bed',\n",
              " 60: 'dining table',\n",
              " 61: 'toilet',\n",
              " 62: 'tv',\n",
              " 63: 'laptop',\n",
              " 64: 'mouse',\n",
              " 65: 'remote',\n",
              " 66: 'keyboard',\n",
              " 67: 'cell phone',\n",
              " 68: 'microwave',\n",
              " 69: 'oven',\n",
              " 70: 'toaster',\n",
              " 71: 'sink',\n",
              " 72: 'refrigerator',\n",
              " 73: 'book',\n",
              " 74: 'clock',\n",
              " 75: 'vase',\n",
              " 76: 'scissors',\n",
              " 77: 'teddy bear',\n",
              " 78: 'hair drier',\n",
              " 79: 'toothbrush'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_names = model.names\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4765017-c28b-4fdd-8fa1-f3b96d9e50ef",
      "metadata": {
        "id": "e4765017-c28b-4fdd-8fa1-f3b96d9e50ef"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(r'C:\\Users\\DELL\\Downloads\\853889-hd_1920_1080_25fps.mp4')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5e117f0-489d-451e-bc1b-0369a3591901",
      "metadata": {
        "id": "a5e117f0-489d-451e-bc1b-0369a3591901"
      },
      "source": [
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO tracking on the frame\n",
        "    results = model.track(frame, persist=True)\n",
        "    print(results)\n",
        "\n",
        "    # Ensure results are not empty\n",
        "    if results[0].boxes.data is not None:\n",
        "        # Get the detected boxes, their class indices, and track IDs\n",
        "        boxes = results[0].boxes.xyxy.cpu()\n",
        "        if results[0].boxes.id is not None:\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "        else:\n",
        "            track_ids = []  # Handle as needed\n",
        "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
        "        confidences = results[0].boxes.conf.cpu()\n",
        "\n",
        "        # Loop through each detected object\n",
        "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            cx = (x1 + x2) // 2  \n",
        "            cy = (y1 + y2) // 2            \n",
        "\n",
        "            class_name = class_names[class_idx]\n",
        "\n",
        "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)           \n",
        "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            \n",
        "        cv2.imshow('yolo_tracking', frame)\n",
        "    \n",
        "        if cv2.waitKey(0) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea5676b4-13f2-4f2a-b327-c73923f9faac",
      "metadata": {
        "id": "ea5676b4-13f2-4f2a-b327-c73923f9faac"
      },
      "outputs": [],
      "source": [
        "# Dictionary to store center points for each track_id\n",
        "track_history = {}\n",
        "\n",
        "# Set the maximum number of frames to track\n",
        "max_frames = 20\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO tracking on the frame\n",
        "    results = model.track(frame, persist=True , classes = [0] )\n",
        "\n",
        "    if results[0].boxes.data is not None:\n",
        "        # Get detected boxes, class indices, and track IDs\n",
        "        boxes = results[0].boxes.xyxy.cpu()\n",
        "        if results[0].boxes.id is not None:\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "        else:\n",
        "            track_ids = []  # Handle as needed\n",
        "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
        "        confidences = results[0].boxes.conf.cpu()\n",
        "\n",
        "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            cx = (x1 + x2) // 2\n",
        "            cy = (y1 + y2) // 2\n",
        "            class_name = class_names[class_idx]\n",
        "\n",
        "            # Update track history\n",
        "            if track_id not in track_history:\n",
        "                track_history[track_id] = []\n",
        "            track_history[track_id].append((cx, cy))\n",
        "\n",
        "            # Keep only the last 20 frames\n",
        "            if len(track_history[track_id]) > max_frames:\n",
        "                track_history[track_id].pop(0)\n",
        "\n",
        "            # Draw center point\n",
        "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
        "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Draw the track history\n",
        "            if len(track_history[track_id]) > 1:\n",
        "                for i in range(1, len(track_history[track_id])):\n",
        "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
        "\n",
        "        cv2.imshow('yolo_tracking', frame)\n",
        "\n",
        "        # Exit on pressing 'q'\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25202dbd-6c99-4b54-97cc-e7f9dd371fa7",
      "metadata": {
        "id": "25202dbd-6c99-4b54-97cc-e7f9dd371fa7",
        "outputId": "8e3f6288-31ea-4fa2-ed19-b89a477c9e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 384x640 26 persons, 2214.7ms\n",
            "Speed: 13.1ms preprocess, 2214.7ms inference, 14.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1490.2ms\n",
            "Speed: 13.0ms preprocess, 1490.2ms inference, 16.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1746.8ms\n",
            "Speed: 8.0ms preprocess, 1746.8ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1870.1ms\n",
            "Speed: 57.2ms preprocess, 1870.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1665.5ms\n",
            "Speed: 13.0ms preprocess, 1665.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1461.3ms\n",
            "Speed: 5.4ms preprocess, 1461.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1400.7ms\n",
            "Speed: 7.0ms preprocess, 1400.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1384.0ms\n",
            "Speed: 6.0ms preprocess, 1384.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1370.4ms\n",
            "Speed: 7.9ms preprocess, 1370.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 3 handbags, 1400.3ms\n",
            "Speed: 3.0ms preprocess, 1400.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 3 handbags, 1365.0ms\n",
            "Speed: 4.4ms preprocess, 1365.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 3 handbags, 1357.6ms\n",
            "Speed: 7.4ms preprocess, 1357.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 3 handbags, 1359.2ms\n",
            "Speed: 6.4ms preprocess, 1359.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1359.6ms\n",
            "Speed: 13.2ms preprocess, 1359.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1369.7ms\n",
            "Speed: 5.9ms preprocess, 1369.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1352.3ms\n",
            "Speed: 2.9ms preprocess, 1352.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1381.2ms\n",
            "Speed: 1.0ms preprocess, 1381.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1413.3ms\n",
            "Speed: 4.7ms preprocess, 1413.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1357.3ms\n",
            "Speed: 3.0ms preprocess, 1357.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1354.6ms\n",
            "Speed: 7.3ms preprocess, 1354.6ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1496.8ms\n",
            "Speed: 3.1ms preprocess, 1496.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1394.0ms\n",
            "Speed: 0.0ms preprocess, 1394.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1360.2ms\n",
            "Speed: 6.8ms preprocess, 1360.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1364.6ms\n",
            "Speed: 7.2ms preprocess, 1364.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1463.8ms\n",
            "Speed: 3.1ms preprocess, 1463.8ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 1 handbag, 1536.2ms\n",
            "Speed: 9.2ms preprocess, 1536.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1481.0ms\n",
            "Speed: 4.7ms preprocess, 1481.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1 handbag, 1348.4ms\n",
            "Speed: 3.2ms preprocess, 1348.4ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1372.8ms\n",
            "Speed: 3.2ms preprocess, 1372.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1338.6ms\n",
            "Speed: 3.3ms preprocess, 1338.6ms inference, 18.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1353.1ms\n",
            "Speed: 7.6ms preprocess, 1353.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1386.8ms\n",
            "Speed: 7.5ms preprocess, 1386.8ms inference, 13.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1350.3ms\n",
            "Speed: 3.6ms preprocess, 1350.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ],
      "source": [
        "# Dictionary to store center points for each track_id\n",
        "track_history = {}\n",
        "\n",
        "# Set the maximum number of frames to track\n",
        "max_frames = 20\n",
        "\n",
        "# Counters for total objects and different objects\n",
        "total_objects = 0\n",
        "object_type_counts = {}\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO tracking on the frame\n",
        "    results = model.track(frame, persist=True)  # Specify classes if needed\n",
        "\n",
        "    if results[0].boxes.data is not None:\n",
        "        # Get detected boxes, class indices, and track IDs\n",
        "        boxes = results[0].boxes.xyxy.cpu()\n",
        "        if results[0].boxes.id is not None:\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "        else:\n",
        "            track_ids = []  # Handle as needed\n",
        "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
        "        confidences = results[0].boxes.conf.cpu()\n",
        "\n",
        "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            cx = (x1 + x2) // 2\n",
        "            cy = (y1 + y2) // 2\n",
        "            class_name = class_names[class_idx]\n",
        "\n",
        "            # Update total object count\n",
        "            total_objects += 1\n",
        "\n",
        "            # Update count for this class type\n",
        "            if class_name in object_type_counts:\n",
        "                object_type_counts[class_name] += 1\n",
        "            else:\n",
        "                object_type_counts[class_name] = 1\n",
        "\n",
        "            # Update track history\n",
        "            if track_id not in track_history:\n",
        "                track_history[track_id] = []\n",
        "            track_history[track_id].append((cx, cy))\n",
        "\n",
        "            # Keep only the last 20 frames\n",
        "            if len(track_history[track_id]) > max_frames:\n",
        "                track_history[track_id].pop(0)\n",
        "\n",
        "            # Draw center point\n",
        "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
        "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Draw the track history\n",
        "            if len(track_history[track_id]) > 1:\n",
        "                for i in range(1, len(track_history[track_id])):\n",
        "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
        "\n",
        "        # Display counts on the frame\n",
        "        cv2.putText(frame, f\"Total Objects: {total_objects}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        y_offset = 50\n",
        "        for class_name, count in object_type_counts.items():\n",
        "            cv2.putText(frame, f\"{class_name}: {count}\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
        "            y_offset += 20\n",
        "\n",
        "        cv2.imshow('yolo_tracking', frame)\n",
        "\n",
        "        # Exit on pressing 'q'\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06547829-5369-41a9-8977-70c025df4678",
      "metadata": {
        "id": "06547829-5369-41a9-8977-70c025df4678",
        "outputId": "97f1ac04-af88-4196-ae42-526f6ff549d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 384x640 26 persons, 2086.0ms\n",
            "Speed: 12.6ms preprocess, 2086.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1461.3ms\n",
            "Speed: 11.6ms preprocess, 1461.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1345.1ms\n",
            "Speed: 1.0ms preprocess, 1345.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1365.8ms\n",
            "Speed: 5.6ms preprocess, 1365.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1356.8ms\n",
            "Speed: 6.6ms preprocess, 1356.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1391.7ms\n",
            "Speed: 5.5ms preprocess, 1391.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1350.1ms\n",
            "Speed: 7.3ms preprocess, 1350.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1356.5ms\n",
            "Speed: 6.6ms preprocess, 1356.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1381.0ms\n",
            "Speed: 14.0ms preprocess, 1381.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 3 handbags, 1362.9ms\n",
            "Speed: 5.9ms preprocess, 1362.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 3 handbags, 1367.0ms\n",
            "Speed: 3.1ms preprocess, 1367.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 3 handbags, 1352.2ms\n",
            "Speed: 6.6ms preprocess, 1352.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 3 handbags, 1364.7ms\n",
            "Speed: 6.6ms preprocess, 1364.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1463.3ms\n",
            "Speed: 3.2ms preprocess, 1463.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1358.2ms\n",
            "Speed: 5.4ms preprocess, 1358.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1374.2ms\n",
            "Speed: 14.0ms preprocess, 1374.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1360.8ms\n",
            "Speed: 3.1ms preprocess, 1360.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1511.2ms\n",
            "Speed: 6.4ms preprocess, 1511.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1364.9ms\n",
            "Speed: 3.0ms preprocess, 1364.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1353.7ms\n",
            "Speed: 3.1ms preprocess, 1353.7ms inference, 16.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1368.6ms\n",
            "Speed: 2.8ms preprocess, 1368.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1534.1ms\n",
            "Speed: 7.3ms preprocess, 1534.1ms inference, 16.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1367.8ms\n",
            "Speed: 13.8ms preprocess, 1367.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1366.3ms\n",
            "Speed: 6.2ms preprocess, 1366.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1366.0ms\n",
            "Speed: 4.2ms preprocess, 1366.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 1 handbag, 1358.1ms\n",
            "Speed: 3.2ms preprocess, 1358.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1361.4ms\n",
            "Speed: 1.0ms preprocess, 1361.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1 handbag, 1361.4ms\n",
            "Speed: 3.2ms preprocess, 1361.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1355.3ms\n",
            "Speed: 6.6ms preprocess, 1355.3ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1351.4ms\n",
            "Speed: 3.0ms preprocess, 1351.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1363.2ms\n",
            "Speed: 3.3ms preprocess, 1363.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1360.3ms\n",
            "Speed: 1.0ms preprocess, 1360.3ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1357.1ms\n",
            "Speed: 6.4ms preprocess, 1357.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1401.1ms\n",
            "Speed: 3.8ms preprocess, 1401.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1381.8ms\n",
            "Speed: 1.0ms preprocess, 1381.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1370.7ms\n",
            "Speed: 1.0ms preprocess, 1370.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1353.6ms\n",
            "Speed: 3.1ms preprocess, 1353.6ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1339.9ms\n",
            "Speed: 7.2ms preprocess, 1339.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1345.0ms\n",
            "Speed: 18.2ms preprocess, 1345.0ms inference, 16.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1374.1ms\n",
            "Speed: 3.0ms preprocess, 1374.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ],
      "source": [
        "# Dictionary to store center points for each track_id\n",
        "track_history = {}\n",
        "\n",
        "# Set the maximum number of frames to track\n",
        "max_frames = 20\n",
        "\n",
        "# Set to store unique track IDs\n",
        "unique_track_ids = set()\n",
        "\n",
        "# Dictionary to count different objects\n",
        "object_type_counts = {}\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO tracking on the frame\n",
        "    results = model.track(frame, persist=True)  # Specify classes if needed\n",
        "\n",
        "    if results[0].boxes.data is not None:\n",
        "        # Get detected boxes, class indices, and track IDs\n",
        "        boxes = results[0].boxes.xyxy.cpu()\n",
        "        if results[0].boxes.id is not None:\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "        else:\n",
        "            track_ids = []  # Handle as needed\n",
        "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
        "        confidences = results[0].boxes.conf.cpu()\n",
        "\n",
        "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            cx = (x1 + x2) // 2\n",
        "            cy = (y1 + y2) // 2\n",
        "            class_name = class_names[class_idx]\n",
        "\n",
        "            # Update unique track IDs\n",
        "            if track_id not in unique_track_ids:\n",
        "                unique_track_ids.add(track_id)\n",
        "\n",
        "                # Update count for this class type\n",
        "                if class_name in object_type_counts:\n",
        "                    object_type_counts[class_name] += 1\n",
        "                else:\n",
        "                    object_type_counts[class_name] = 1\n",
        "\n",
        "            # Update track history\n",
        "            if track_id not in track_history:\n",
        "                track_history[track_id] = []\n",
        "            track_history[track_id].append((cx, cy))\n",
        "\n",
        "            # Keep only the last 20 frames\n",
        "            if len(track_history[track_id]) > max_frames:\n",
        "                track_history[track_id].pop(0)\n",
        "\n",
        "            # Draw center point\n",
        "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
        "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Draw the track history\n",
        "            if len(track_history[track_id]) > 1:\n",
        "                for i in range(1, len(track_history[track_id])):\n",
        "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
        "\n",
        "        # Display counts on the frame\n",
        "        total_objects = len(unique_track_ids)\n",
        "        cv2.putText(frame, f\"Total Objects: {total_objects}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        y_offset = 50\n",
        "        for class_name, count in object_type_counts.items():\n",
        "            cv2.putText(frame, f\"{class_name}: {count}\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
        "            y_offset += 20\n",
        "\n",
        "        cv2.imshow('yolo_tracking', frame)\n",
        "\n",
        "        # Exit on pressing 'q'\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ba5712f-5aca-4026-bc05-0cb8659b1580",
      "metadata": {
        "id": "3ba5712f-5aca-4026-bc05-0cb8659b1580",
        "outputId": "2c3b95e7-fe70-4986-c34d-104a383ac35b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 384x640 26 persons, 1844.5ms\n",
            "Speed: 15.4ms preprocess, 1844.5ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1470.7ms\n",
            "Speed: 14.0ms preprocess, 1470.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1343.4ms\n",
            "Speed: 15.6ms preprocess, 1343.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1499.7ms\n",
            "Speed: 0.0ms preprocess, 1499.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1405.9ms\n",
            "Speed: 15.6ms preprocess, 1405.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 3 handbags, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 3 handbags, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 3 handbags, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 3 handbags, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1390.3ms\n",
            "Speed: 15.6ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1405.9ms\n",
            "Speed: 0.0ms preprocess, 1405.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1421.5ms\n",
            "Speed: 15.6ms preprocess, 1421.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1405.9ms\n",
            "Speed: 0.0ms preprocess, 1405.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1562.1ms\n",
            "Speed: 0.0ms preprocess, 1562.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 1 handbag, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1 handbag, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1421.5ms\n",
            "Speed: 15.6ms preprocess, 1421.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1390.3ms\n",
            "Speed: 15.6ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1546.5ms\n",
            "Speed: 15.6ms preprocess, 1546.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1343.4ms\n",
            "Speed: 15.6ms preprocess, 1343.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1 tennis racket, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1 tennis racket, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1 handbag, 1 tennis racket, 1374.6ms\n",
            "Speed: 15.7ms preprocess, 1374.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 1405.9ms\n",
            "Speed: 15.6ms preprocess, 1405.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 2 handbags, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 backpack, 3 handbags, 1 tennis racket, 1452.8ms\n",
            "Speed: 15.6ms preprocess, 1452.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 backpack, 3 handbags, 1 tennis racket, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 backpack, 2 handbags, 1 tennis racket, 1484.0ms\n",
            "Speed: 0.0ms preprocess, 1484.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 backpack, 3 handbags, 1 tennis racket, 1405.9ms\n",
            "Speed: 0.0ms preprocess, 1405.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 backpack, 2 handbags, 1 tennis racket, 1484.0ms\n",
            "Speed: 0.0ms preprocess, 1484.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 backpack, 2 handbags, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 backpack, 1 handbag, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 backpack, 1 handbag, 1624.6ms\n",
            "Speed: 15.6ms preprocess, 1624.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 backpack, 1 handbag, 1421.5ms\n",
            "Speed: 0.0ms preprocess, 1421.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 backpack, 1 handbag, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 backpack, 1 handbag, 1 tennis racket, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 backpack, 1 handbag, 1 tennis racket, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 backpack, 1 handbag, 1 tennis racket, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 backpack, 1 handbag, 1 tennis racket, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1 backpack, 2 handbags, 1 tennis racket, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1 backpack, 2 handbags, 1 tennis racket, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 backpack, 3 handbags, 1 tennis racket, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 2 backpacks, 2 handbags, 1 tennis racket, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 backpack, 3 handbags, 1 tennis racket, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 3 handbags, 1 tennis racket, 1421.5ms\n",
            "Speed: 0.0ms preprocess, 1421.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 3 handbags, 1 tennis racket, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 3 handbags, 1 tennis racket, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 handbag, 1 tennis racket, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 handbag, 1 tennis racket, 1343.4ms\n",
            "Speed: 0.0ms preprocess, 1343.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 handbag, 1 tennis racket, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 handbag, 2 tennis rackets, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 handbag, 2 tennis rackets, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 handbag, 2 tennis rackets, 1562.1ms\n",
            "Speed: 0.0ms preprocess, 1562.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 handbag, 2 tennis rackets, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import time\n",
        "\n",
        "track_history = {}\n",
        "\n",
        "# Define the circle parameters (x, y, radius)\n",
        "circle_center = (1021, 164)  # Example circle center\n",
        "circle_radius = 100  # Example radius of the circle\n",
        "\n",
        "# Dictionary to store the time spent in the circle for each track_id\n",
        "time_in_circle = {}\n",
        "\n",
        "# Dictionary to store people who are inside the circle\n",
        "people_in_circle = set()\n",
        "\n",
        "# Set the maximum number of frames to track\n",
        "max_frames = 20\n",
        "\n",
        "# Set to store unique track IDs\n",
        "unique_track_ids = set()\n",
        "\n",
        "# Dictionary to count different objects\n",
        "object_type_counts = {}\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO tracking on the frame\n",
        "    results = model.track(frame, persist=True)  # Specify classes if needed\n",
        "\n",
        "    if results[0].boxes.data is not None:\n",
        "        # Get detected boxes, class indices, and track IDs\n",
        "        boxes = results[0].boxes.xyxy.cpu()\n",
        "        if results[0].boxes.id is not None:\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "        else:\n",
        "            track_ids = []  # Handle as needed\n",
        "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
        "        confidences = results[0].boxes.conf.cpu()\n",
        "\n",
        "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            cx = (x1 + x2) // 2\n",
        "            cy = (y1 + y2) // 2\n",
        "            class_name = class_names[class_idx]\n",
        "\n",
        "            # Update unique track IDs\n",
        "            if track_id not in unique_track_ids:\n",
        "                unique_track_ids.add(track_id)\n",
        "\n",
        "                # Update count for this class type\n",
        "                if class_name in object_type_counts:\n",
        "                    object_type_counts[class_name] += 1\n",
        "                else:\n",
        "                    object_type_counts[class_name] = 1\n",
        "\n",
        "            # Update track history\n",
        "            if track_id not in track_history:\n",
        "                track_history[track_id] = []\n",
        "            track_history[track_id].append((cx, cy))\n",
        "\n",
        "            # Keep only the last 20 frames\n",
        "            if len(track_history[track_id]) > max_frames:\n",
        "                track_history[track_id].pop(0)\n",
        "\n",
        "            # Check if the person is inside the circle\n",
        "            distance_to_center = ((cx - circle_center[0]) ** 2 + (cy - circle_center[1]) ** 2) ** 0.5\n",
        "            if distance_to_center <= circle_radius:\n",
        "                if track_id not in people_in_circle:\n",
        "                    people_in_circle.add(track_id)\n",
        "                    time_in_circle[track_id] = time.time()  # Track the entry time for new people\n",
        "\n",
        "            # If the person is inside the circle, calculate the time spent\n",
        "            if track_id in people_in_circle:\n",
        "                time_spent = time.time() - time_in_circle[track_id]\n",
        "                minutes_spent = time_spent / 60  # Convert seconds to minutes\n",
        "                cv2.putText(frame, f\"Time in Circle: {minutes_spent:.2f} min\", (x1, y1 - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "\n",
        "            # Draw center point and track information\n",
        "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
        "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Draw the track history\n",
        "            if len(track_history[track_id]) > 1:\n",
        "                for i in range(1, len(track_history[track_id])):\n",
        "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
        "\n",
        "        # Display counts on the frame\n",
        "        total_objects = len(unique_track_ids)\n",
        "        cv2.putText(frame, f\"Total Objects: {total_objects}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        y_offset = 50\n",
        "        for class_name, count in object_type_counts.items():\n",
        "            cv2.putText(frame, f\"{class_name}: {count}\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
        "            y_offset += 20\n",
        "\n",
        "        # Draw the circle on the frame\n",
        "        cv2.circle(frame, circle_center, circle_radius, (0, 0, 255), 2)\n",
        "\n",
        "        cv2.imshow('yolo_tracking', frame)\n",
        "\n",
        "        # Exit on pressing 'q'\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09752b44-0bdd-42c8-bd63-deb411abac03",
      "metadata": {
        "id": "09752b44-0bdd-42c8-bd63-deb411abac03"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Initialize variables\n",
        "circle_radius = 100\n",
        "circle_center = None\n",
        "\n",
        "# Mouse callback function to set the center of the circle\n",
        "def set_circle_position(event, x, y, flags, param):\n",
        "    global circle_center\n",
        "    if event == cv2.EVENT_LBUTTONDOWN:  # When left mouse button is clicked\n",
        "        circle_center = (x, y)  # Set the center of the circle to the click position\n",
        "\n",
        "# Create a window and bind the mouse callback function\n",
        "cv2.namedWindow('Select Circle Position')\n",
        "cv2.setMouseCallback('Select Circle Position', set_circle_position)\n",
        "\n",
        "# Start video capture (or any frame source)\n",
        "cap = cv2.VideoCapture(0)  # You can replace this with your video stream source\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if circle_center:\n",
        "        # Draw the circle on the frame at the selected position\n",
        "        cv2.circle(frame, circle_center, circle_radius, (0, 0, 255), 2)\n",
        "\n",
        "    # Show the frame with circle\n",
        "    cv2.imshow('Select Circle Position', frame)\n",
        "\n",
        "    # Exit on pressing 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4698adbb-34f7-41e0-b633-da59985e0260",
      "metadata": {
        "id": "4698adbb-34f7-41e0-b633-da59985e0260",
        "outputId": "b7256307-0bab-49bf-ff3d-89e90c0e459b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Circle Center: (1021, 164)\n",
            "Selected Circle Center: (425, 512)\n",
            "Selected Circle Center: (1204, 60)\n",
            "Selected Circle Center: (940, 502)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "# Initialize variables\n",
        "circle_radius = 100\n",
        "circle_center = None\n",
        "\n",
        "# Mouse callback function to set the center of the circle\n",
        "def set_circle_position(event, x, y, flags, param):\n",
        "    global circle_center\n",
        "    if event == cv2.EVENT_LBUTTONDOWN:  # When left mouse button is clicked\n",
        "        circle_center = (x, y)  # Set the center of the circle to the click position\n",
        "        print(f\"Selected Circle Center: {circle_center}\")  # Print the selected position\n",
        "\n",
        "# Create a window and bind the mouse callback function\n",
        "cv2.namedWindow('Select Circle Position')\n",
        "cv2.setMouseCallback('Select Circle Position', set_circle_position)\n",
        "\n",
        "# Start video capture (use a video file instead of the webcam)\n",
        "video_path = r'C:\\Users\\DELL\\Downloads\\853889-hd_1920_1080_25fps.mp4'  # Replace with your video file path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if circle_center:\n",
        "        # Draw the circle on the frame at the selected position\n",
        "        cv2.circle(frame, circle_center, circle_radius, (0, 0, 255), 2)\n",
        "\n",
        "    # Show the frame with circle\n",
        "    cv2.imshow('Select Circle Position', frame)\n",
        "\n",
        "    # Exit on pressing 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972a0f7d-08b5-4525-ab3d-d5fd745aa396",
      "metadata": {
        "id": "972a0f7d-08b5-4525-ab3d-d5fd745aa396"
      },
      "outputs": [],
      "source": [
        "# Dictionary to store center points for each track_id\n",
        "track_history = {}\n",
        "\n",
        "# Set the maximum number of frames to track\n",
        "max_frames = 20\n",
        "\n",
        "# Set to store unique track IDs\n",
        "unique_track_ids = set()\n",
        "\n",
        "# Dictionary to count different objects\n",
        "object_type_counts = {}\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO tracking on the frame\n",
        "    results = model.track(frame, persist=True)  # Specify classes if needed\n",
        "\n",
        "    if results[0].boxes.data is not None:\n",
        "        # Get detected boxes, class indices, and track IDs\n",
        "        boxes = results[0].boxes.xyxy.cpu()\n",
        "        if results[0].boxes.id is not None:\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "        else:\n",
        "            track_ids = []  # Handle as needed\n",
        "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
        "        confidences = results[0].boxes.conf.cpu()\n",
        "\n",
        "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            cx = (x1 + x2) // 2\n",
        "            cy = (y1 + y2) // 2\n",
        "            class_name = class_names[class_idx]\n",
        "\n",
        "            # Update unique track IDs\n",
        "            if track_id not in unique_track_ids:\n",
        "                unique_track_ids.add(track_id)\n",
        "\n",
        "                # Update count for this class type\n",
        "                if class_name in object_type_counts:\n",
        "                    object_type_counts[class_name] += 1\n",
        "                else:\n",
        "                    object_type_counts[class_name] = 1\n",
        "\n",
        "            # Update track history\n",
        "            if track_id not in track_history:\n",
        "                track_history[track_id] = []\n",
        "            track_history[track_id].append((cx, cy))\n",
        "\n",
        "            # Keep only the last 20 frames\n",
        "            if len(track_history[track_id]) > max_frames:\n",
        "                track_history[track_id].pop(0)\n",
        "\n",
        "            # Draw center point\n",
        "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
        "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Draw the track history\n",
        "            if len(track_history[track_id]) > 1:\n",
        "                for i in range(1, len(track_history[track_id])):\n",
        "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
        "\n",
        "        # Display counts on the frame\n",
        "        total_objects = len(unique_track_ids)\n",
        "        cv2.putText(frame, f\"Total Objects: {total_objects}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        y_offset = 50\n",
        "        for class_name, count in object_type_counts.items():\n",
        "            cv2.putText(frame, f\"{class_name}: {count}\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
        "            y_offset += 20\n",
        "\n",
        "        cv2.imshow('yolo_tracking', frame)\n",
        "\n",
        "        # Exit on pressing 'q'\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab50b9e8-e6c3-4b51-896d-bac3a5b39bc6",
      "metadata": {
        "id": "ab50b9e8-e6c3-4b51-896d-bac3a5b39bc6",
        "outputId": "47e5c00c-8213-4282-dc7b-a0cdda329185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 384x640 16 persons, 1595.5ms\n",
            "Speed: 6.7ms preprocess, 1595.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 persons, 1370.6ms\n",
            "Speed: 5.4ms preprocess, 1370.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 persons, 1348.3ms\n",
            "Speed: 6.8ms preprocess, 1348.3ms inference, 16.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 persons, 1368.9ms\n",
            "Speed: 6.4ms preprocess, 1368.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 persons, 1363.5ms\n",
            "Speed: 6.6ms preprocess, 1363.5ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 persons, 1350.6ms\n",
            "Speed: 5.9ms preprocess, 1350.6ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 persons, 1 handbag, 1366.0ms\n",
            "Speed: 5.9ms preprocess, 1366.0ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 persons, 1 handbag, 1366.9ms\n",
            "Speed: 5.8ms preprocess, 1366.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 handbag, 1350.7ms\n",
            "Speed: 8.2ms preprocess, 1350.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 3 handbags, 1339.8ms\n",
            "Speed: 16.6ms preprocess, 1339.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 3 handbags, 1355.2ms\n",
            "Speed: 5.2ms preprocess, 1355.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 2 handbags, 1348.2ms\n",
            "Speed: 8.0ms preprocess, 1348.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 3 handbags, 1373.1ms\n",
            "Speed: 2.7ms preprocess, 1373.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 2 handbags, 1355.9ms\n",
            "Speed: 6.5ms preprocess, 1355.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1373.1ms\n",
            "Speed: 4.5ms preprocess, 1373.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1340.9ms\n",
            "Speed: 6.2ms preprocess, 1340.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1409.1ms\n",
            "Speed: 17.2ms preprocess, 1409.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1344.8ms\n",
            "Speed: 5.1ms preprocess, 1344.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1353.5ms\n",
            "Speed: 4.3ms preprocess, 1353.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1348.8ms\n",
            "Speed: 4.6ms preprocess, 1348.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1566.9ms\n",
            "Speed: 0.0ms preprocess, 1566.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1364.3ms\n",
            "Speed: 7.7ms preprocess, 1364.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 1 handbag, 1362.7ms\n",
            "Speed: 2.9ms preprocess, 1362.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 1 handbag, 1364.1ms\n",
            "Speed: 5.7ms preprocess, 1364.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 1 handbag, 1356.4ms\n",
            "Speed: 5.3ms preprocess, 1356.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 1 handbag, 1345.2ms\n",
            "Speed: 5.9ms preprocess, 1345.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 persons, 1353.0ms\n",
            "Speed: 3.1ms preprocess, 1353.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 1 handbag, 1360.3ms\n",
            "Speed: 3.4ms preprocess, 1360.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 persons, 1 handbag, 1364.6ms\n",
            "Speed: 2.0ms preprocess, 1364.6ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 2 handbags, 1360.3ms\n",
            "Speed: 4.0ms preprocess, 1360.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 2 handbags, 1 tennis racket, 1361.8ms\n",
            "Speed: 5.4ms preprocess, 1361.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1358.4ms\n",
            "Speed: 1.8ms preprocess, 1358.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1337.9ms\n",
            "Speed: 4.9ms preprocess, 1337.9ms inference, 17.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1364.3ms\n",
            "Speed: 0.0ms preprocess, 1364.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1360.2ms\n",
            "Speed: 3.6ms preprocess, 1360.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1365.0ms\n",
            "Speed: 3.0ms preprocess, 1365.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1394.7ms\n",
            "Speed: 4.2ms preprocess, 1394.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1366.2ms\n",
            "Speed: 4.1ms preprocess, 1366.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1362.0ms\n",
            "Speed: 1.6ms preprocess, 1362.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1384.8ms\n",
            "Speed: 0.0ms preprocess, 1384.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1542.3ms\n",
            "Speed: 0.0ms preprocess, 1542.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1360.8ms\n",
            "Speed: 6.7ms preprocess, 1360.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1352.7ms\n",
            "Speed: 7.6ms preprocess, 1352.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1348.1ms\n",
            "Speed: 3.2ms preprocess, 1348.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 1 handbag, 1338.4ms\n",
            "Speed: 18.9ms preprocess, 1338.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 1 handbag, 1345.8ms\n",
            "Speed: 17.4ms preprocess, 1345.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 1353.1ms\n",
            "Speed: 1.0ms preprocess, 1353.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 1369.4ms\n",
            "Speed: 1.0ms preprocess, 1369.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1340.8ms\n",
            "Speed: 4.3ms preprocess, 1340.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1 handbag, 1340.8ms\n",
            "Speed: 14.9ms preprocess, 1340.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1 handbag, 1397.4ms\n",
            "Speed: 7.2ms preprocess, 1397.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1348.8ms\n",
            "Speed: 16.7ms preprocess, 1348.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 2 handbags, 1358.3ms\n",
            "Speed: 3.1ms preprocess, 1358.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 1 backpack, 1 handbag, 1351.2ms\n",
            "Speed: 4.1ms preprocess, 1351.2ms inference, 15.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 handbag, 1349.0ms\n",
            "Speed: 6.8ms preprocess, 1349.0ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 persons, 1358.7ms\n",
            "Speed: 5.2ms preprocess, 1358.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 persons, 1355.8ms\n",
            "Speed: 3.2ms preprocess, 1355.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 persons, 1357.7ms\n",
            "Speed: 3.5ms preprocess, 1357.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 persons, 1353.6ms\n",
            "Speed: 3.2ms preprocess, 1353.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 persons, 1355.1ms\n",
            "Speed: 2.6ms preprocess, 1355.1ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 persons, 1552.5ms\n",
            "Speed: 7.4ms preprocess, 1552.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1 handbag, 1399.0ms\n",
            "Speed: 2.9ms preprocess, 1399.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 1 handbag, 1346.7ms\n",
            "Speed: 6.4ms preprocess, 1346.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 1 handbag, 1 tennis racket, 1345.2ms\n",
            "Speed: 3.1ms preprocess, 1345.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1341.8ms\n",
            "Speed: 3.1ms preprocess, 1341.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1349.7ms\n",
            "Speed: 3.3ms preprocess, 1349.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1356.5ms\n",
            "Speed: 3.8ms preprocess, 1356.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 3 handbags, 1 tennis racket, 1393.2ms\n",
            "Speed: 3.3ms preprocess, 1393.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 1354.5ms\n",
            "Speed: 3.1ms preprocess, 1354.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 backpack, 3 handbags, 1 tennis racket, 1352.4ms\n",
            "Speed: 3.1ms preprocess, 1352.4ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 backpack, 3 handbags, 1 tennis racket, 1387.4ms\n",
            "Speed: 2.9ms preprocess, 1387.4ms inference, 17.8ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "# Initial confidence threshold\n",
        "confidence_threshold = 50  # Percentage (0-100)\n",
        "\n",
        "# Callback function for the trackbar\n",
        "def update_confidence(x):\n",
        "    global confidence_threshold\n",
        "    confidence_threshold = x / 100  # Convert to decimal (0.0-1.0)\n",
        "\n",
        "# Create a window\n",
        "cv2.namedWindow('yolo_tracking')\n",
        "\n",
        "# Create the trackbar\n",
        "cv2.createTrackbar('Confidence', 'yolo_tracking', 50, 100, update_confidence)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO tracking with the current confidence threshold\n",
        "    results = model.track(frame, persist=True, conf=confidence_threshold)\n",
        "\n",
        "    if results[0].boxes.data is not None:\n",
        "        boxes = results[0].boxes.xyxy.cpu()\n",
        "        if results[0].boxes.id is not None:\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "        else:\n",
        "            track_ids = []\n",
        "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
        "        confidences = results[0].boxes.conf.cpu()\n",
        "\n",
        "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            cx = (x1 + x2) // 2\n",
        "            cy = (y1 + y2) // 2\n",
        "            class_name = class_names[class_idx]\n",
        "\n",
        "            # Draw center point\n",
        "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
        "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "    # Display the current confidence threshold\n",
        "    cv2.putText(frame, f\"Confidence: {confidence_threshold:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "    cv2.imshow('yolo_tracking', frame)\n",
        "\n",
        "    # Exit on pressing 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6667b0a6-03c8-4eda-917d-6ba13bd46249",
      "metadata": {
        "id": "6667b0a6-03c8-4eda-917d-6ba13bd46249",
        "outputId": "a21eacb6-8afa-495f-e8e5-3450d9a3b561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 384x640 16 persons, 1598.6ms\n",
            "Speed: 6.8ms preprocess, 1598.6ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 persons, 1352.7ms\n",
            "Speed: 3.3ms preprocess, 1352.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 persons, 1357.5ms\n",
            "Speed: 3.1ms preprocess, 1357.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 persons, 1344.2ms\n",
            "Speed: 6.3ms preprocess, 1344.2ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 persons, 1358.1ms\n",
            "Speed: 6.5ms preprocess, 1358.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 persons, 1344.2ms\n",
            "Speed: 5.5ms preprocess, 1344.2ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 persons, 1 handbag, 1373.9ms\n",
            "Speed: 7.9ms preprocess, 1373.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 persons, 1344.2ms\n",
            "Speed: 6.6ms preprocess, 1344.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 persons, 1355.5ms\n",
            "Speed: 16.5ms preprocess, 1355.5ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 persons, 1372.2ms\n",
            "Speed: 10.0ms preprocess, 1372.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 persons, 1354.7ms\n",
            "Speed: 7.5ms preprocess, 1354.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 persons, 1351.6ms\n",
            "Speed: 3.0ms preprocess, 1351.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 persons, 1334.1ms\n",
            "Speed: 5.6ms preprocess, 1334.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 persons, 1351.2ms\n",
            "Speed: 5.9ms preprocess, 1351.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 persons, 1359.5ms\n",
            "Speed: 1.0ms preprocess, 1359.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 persons, 1338.7ms\n",
            "Speed: 12.8ms preprocess, 1338.7ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 persons, 1337.4ms\n",
            "Speed: 8.6ms preprocess, 1337.4ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 1362.7ms\n",
            "Speed: 6.2ms preprocess, 1362.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 17 persons, 1 handbag, 1361.3ms\n",
            "Speed: 3.3ms preprocess, 1361.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 16 persons, 1342.8ms\n",
            "Speed: 4.3ms preprocess, 1342.8ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 persons, 1591.6ms\n",
            "Speed: 2.9ms preprocess, 1591.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 persons, 1541.1ms\n",
            "Speed: 2.9ms preprocess, 1541.1ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 persons, 1412.8ms\n",
            "Speed: 12.2ms preprocess, 1412.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1352.0ms\n",
            "Speed: 1.0ms preprocess, 1352.0ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 1366.0ms\n",
            "Speed: 2.8ms preprocess, 1366.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 1358.1ms\n",
            "Speed: 11.6ms preprocess, 1358.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 1 handbag, 1360.8ms\n",
            "Speed: 2.7ms preprocess, 1360.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 1 handbag, 1351.2ms\n",
            "Speed: 16.4ms preprocess, 1351.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 1 handbag, 1350.6ms\n",
            "Speed: 3.0ms preprocess, 1350.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1358.7ms\n",
            "Speed: 1.0ms preprocess, 1358.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 2 handbags, 1371.1ms\n",
            "Speed: 1.0ms preprocess, 1371.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1346.8ms\n",
            "Speed: 3.3ms preprocess, 1346.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 1 tennis racket, 1359.3ms\n",
            "Speed: 3.3ms preprocess, 1359.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 persons, 2 handbags, 1 tennis racket, 1341.7ms\n",
            "Speed: 3.1ms preprocess, 1341.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 persons, 2 handbags, 1 tennis racket, 1352.4ms\n",
            "Speed: 3.1ms preprocess, 1352.4ms inference, 14.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1370.5ms\n",
            "Speed: 2.2ms preprocess, 1370.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1372.4ms\n",
            "Speed: 1.0ms preprocess, 1372.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1328.7ms\n",
            "Speed: 5.5ms preprocess, 1328.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 2 handbags, 1 tennis racket, 1355.9ms\n",
            "Speed: 3.0ms preprocess, 1355.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1427.0ms\n",
            "Speed: 6.4ms preprocess, 1427.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1559.4ms\n",
            "Speed: 3.6ms preprocess, 1559.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 1352.3ms\n",
            "Speed: 9.4ms preprocess, 1352.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1358.6ms\n",
            "Speed: 3.2ms preprocess, 1358.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 2 handbags, 1 tennis racket, 1338.6ms\n",
            "Speed: 3.1ms preprocess, 1338.6ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 22 persons, 1 handbag, 1370.8ms\n",
            "Speed: 4.1ms preprocess, 1370.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 1 handbag, 1355.7ms\n",
            "Speed: 3.0ms preprocess, 1355.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 1413.6ms\n",
            "Speed: 7.2ms preprocess, 1413.6ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 21 persons, 1350.9ms\n",
            "Speed: 6.7ms preprocess, 1350.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1489.1ms\n",
            "Speed: 3.0ms preprocess, 1489.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1359.5ms\n",
            "Speed: 3.1ms preprocess, 1359.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1364.1ms\n",
            "Speed: 1.0ms preprocess, 1364.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 24 persons, 1 handbag, 1343.9ms\n",
            "Speed: 4.9ms preprocess, 1343.9ms inference, 16.7ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import torch\n",
        "\n",
        "# Initial confidence and NMS thresholds (in percentage)\n",
        "confidence_threshold = 50  # Confidence threshold in percentage (0-100)\n",
        "nms_threshold = 40  # NMS threshold in percentage (0-100)\n",
        "\n",
        "# Callback function for the confidence trackbar\n",
        "def update_confidence(x):\n",
        "    global confidence_threshold\n",
        "    confidence_threshold = x / 100  # Convert to decimal (0.0-1.0)\n",
        "\n",
        "# Callback function for the NMS trackbar\n",
        "def update_nms(x):\n",
        "    global nms_threshold\n",
        "    nms_threshold = x / 100  # Convert to decimal (0.0-1.0)\n",
        "\n",
        "# Create a window for displaying the video and parameters\n",
        "cv2.namedWindow('yolo_tracking')\n",
        "\n",
        "# Create trackbars for confidence and NMS thresholds\n",
        "cv2.createTrackbar('Confidence', 'yolo_tracking', 50, 100, update_confidence)\n",
        "cv2.createTrackbar('NMS Threshold', 'yolo_tracking', 40, 100, update_nms)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO tracking with current confidence and NMS thresholds\n",
        "    results = model.track(frame, persist=True, conf=confidence_threshold)\n",
        "\n",
        "    if results[0].boxes.data is not None:\n",
        "        boxes = results[0].boxes.xyxy.cpu()\n",
        "        if results[0].boxes.id is not None:\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "        else:\n",
        "            track_ids = []\n",
        "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
        "        confidences = results[0].boxes.conf.cpu()\n",
        "\n",
        "        # Apply NMS with the current NMS threshold\n",
        "        indices = cv2.dnn.NMSBoxes(boxes.tolist(), confidences.tolist(), score_threshold=confidence_threshold, nms_threshold=nms_threshold)\n",
        "\n",
        "        for i in indices.flatten():\n",
        "            x1, y1, x2, y2 = map(int, boxes[i])\n",
        "            cx = (x1 + x2) // 2\n",
        "            cy = (y1 + y2) // 2\n",
        "            class_name = class_names[class_indices[i]]\n",
        "\n",
        "            # Draw center point\n",
        "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
        "            cv2.putText(frame, f\"ID: {track_ids[i]} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "    # Display the current confidence and NMS threshold values\n",
        "    cv2.putText(frame, f\"Confidence: {confidence_threshold:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "    cv2.putText(frame, f\"NMS Threshold: {nms_threshold:.2f}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "    # Show the frame with the bounding boxes and tracking\n",
        "    cv2.imshow('yolo_tracking', frame)\n",
        "\n",
        "    # Exit on pressing 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "860fa7ec-9ff0-41aa-b1e1-d64f265e0486",
      "metadata": {
        "id": "860fa7ec-9ff0-41aa-b1e1-d64f265e0486"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "\n",
        "# Initial confidence and NMS thresholds (in percentage)\n",
        "confidence_threshold = 50  # Confidence threshold in percentage (0-100)\n",
        "nms_threshold = 40  # NMS threshold in percentage (0-100)\n",
        "\n",
        "# Callback function for the confidence trackbar\n",
        "def update_confidence(x):\n",
        "    global confidence_threshold\n",
        "    confidence_threshold = x / 100  # Convert to decimal (0.0-1.0)\n",
        "\n",
        "# Callback function for the NMS trackbar\n",
        "def update_nms(x):\n",
        "    global nms_threshold\n",
        "    nms_threshold = x / 100  # Convert to decimal (0.0-1.0)\n",
        "\n",
        "# Create a window for displaying the video and parameters\n",
        "cv2.namedWindow('yolo_tracking')\n",
        "\n",
        "# Create trackbars for confidence and NMS thresholds\n",
        "cv2.createTrackbar('Confidence', 'yolo_tracking', 50, 100, update_confidence)\n",
        "cv2.createTrackbar('NMS Threshold', 'yolo_tracking', 40, 100, update_nms)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO tracking with current confidence and NMS thresholds\n",
        "    results = model.track(frame, persist=True, conf=confidence_threshold)\n",
        "\n",
        "    if results[0].boxes.data is not None:\n",
        "        boxes = results[0].boxes.xyxy.cpu()\n",
        "        if results[0].boxes.id is not None:\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "        else:\n",
        "            track_ids = []\n",
        "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
        "        confidences = results[0].boxes.conf.cpu()\n",
        "\n",
        "        # Apply NMS with the current NMS threshold\n",
        "        indices = cv2.dnn.NMSBoxes(boxes.tolist(), confidences.tolist(), score_threshold=confidence_threshold, nms_threshold=nms_threshold)\n",
        "\n",
        "        for i in indices.flatten():\n",
        "            x1, y1, x2, y2 = map(int, boxes[i])\n",
        "            cx = (x1 + x2) // 2\n",
        "            cy = (y1 + y2) // 2\n",
        "            class_name = class_names[class_indices[i]]\n",
        "\n",
        "            # Draw center point\n",
        "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
        "            cv2.putText(frame, f\"ID: {track_ids[i]} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "    # Display the current confidence and NMS threshold values\n",
        "    cv2.putText(frame, f\"Confidence: {confidence_threshold:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "    cv2.putText(frame, f\"NMS Threshold: {nms_threshold:.2f}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "    # Show the frame with the bounding boxes and tracking\n",
        "    cv2.imshow('yolo_tracking', frame)\n",
        "\n",
        "    # Exit on pressing 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f1fd917-3fa8-4ad1-8ccd-b03723f9e922",
      "metadata": {
        "id": "0f1fd917-3fa8-4ad1-8ccd-b03723f9e922"
      },
      "outputs": [],
      "source": [
        "# Dictionary to store center points for each track_id\n",
        "track_history = {}\n",
        "\n",
        "# Set the maximum number of frames to track\n",
        "max_frames = 20\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO tracking on the frame\n",
        "    results = model.track(frame, persist=True , classes = [0] )\n",
        "\n",
        "    if results[0].boxes.data is not None:\n",
        "        # Get detected boxes, class indices, and track IDs\n",
        "        boxes = results[0].boxes.xyxy.cpu()\n",
        "        if results[0].boxes.id is not None:\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "        else:\n",
        "            track_ids = []  # Handle as needed\n",
        "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
        "        confidences = results[0].boxes.conf.cpu()\n",
        "\n",
        "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            cx = (x1 + x2) // 2\n",
        "            cy = (y1 + y2) // 2\n",
        "            class_name = class_names[class_idx]\n",
        "\n",
        "            # Update track history\n",
        "            if track_id not in track_history:\n",
        "                track_history[track_id] = []\n",
        "            track_history[track_id].append((cx, cy))\n",
        "\n",
        "            # Keep only the last 20 frames\n",
        "            if len(track_history[track_id]) > max_frames:\n",
        "                track_history[track_id].pop(0)\n",
        "\n",
        "            # Draw center point\n",
        "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
        "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Draw the track history\n",
        "            if len(track_history[track_id]) > 1:\n",
        "                for i in range(1, len(track_history[track_id])):\n",
        "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
        "\n",
        "        cv2.imshow('yolo_tracking', frame)\n",
        "\n",
        "        # Exit on pressing 'q'\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d321ca-95c4-4cbe-9084-9ef82a51373c",
      "metadata": {
        "id": "97d321ca-95c4-4cbe-9084-9ef82a51373c",
        "outputId": "d27e5f11-c9fb-4285-a3bb-dbff8d7159cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 384x640 28 persons, 1768.6ms\n",
            "Speed: 10.0ms preprocess, 1768.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1426.6ms\n",
            "Speed: 15.8ms preprocess, 1426.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Entry detected: ID 20\n",
            "\n",
            "0: 384x640 28 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1405.9ms\n",
            "Speed: 0.0ms preprocess, 1405.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1530.9ms\n",
            "Speed: 15.6ms preprocess, 1530.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1343.4ms\n",
            "Speed: 15.6ms preprocess, 1343.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1343.4ms\n",
            "Speed: 15.6ms preprocess, 1343.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1343.4ms\n",
            "Speed: 15.6ms preprocess, 1343.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1468.4ms\n",
            "Speed: 0.0ms preprocess, 1468.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1499.7ms\n",
            "Speed: 15.6ms preprocess, 1499.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Exit detected: ID 3\n",
            "\n",
            "0: 384x640 26 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1343.4ms\n",
            "Speed: 15.6ms preprocess, 1343.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1546.5ms\n",
            "Speed: 15.6ms preprocess, 1546.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Exit detected: ID 18\n",
            "\n",
            "0: 384x640 26 persons, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.0ms\n",
            "Speed: 0.0ms preprocess, 1359.0ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1546.5ms\n",
            "Speed: 15.6ms preprocess, 1546.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1343.5ms\n",
            "Speed: 0.0ms preprocess, 1343.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1546.5ms\n",
            "Speed: 15.6ms preprocess, 1546.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1359.0ms\n",
            "Speed: 0.0ms preprocess, 1359.0ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1343.4ms\n",
            "Speed: 0.0ms preprocess, 1343.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1343.5ms\n",
            "Speed: 15.6ms preprocess, 1343.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1390.3ms\n",
            "Speed: 15.6ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1374.6ms\n",
            "Speed: 0.0ms preprocess, 1374.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Exit detected: ID 1\n",
            "\n",
            "0: 384x640 30 persons, 1343.4ms\n",
            "Speed: 15.6ms preprocess, 1343.4ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1374.7ms\n",
            "Speed: 15.6ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1546.5ms\n",
            "Speed: 0.0ms preprocess, 1546.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1390.3ms\n",
            "Speed: 15.6ms preprocess, 1390.3ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1468.4ms\n",
            "Speed: 0.0ms preprocess, 1468.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1405.9ms\n",
            "Speed: 0.0ms preprocess, 1405.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Entry detected: ID 56\n",
            "\n",
            "0: 384x640 30 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1359.0ms\n",
            "Speed: 15.6ms preprocess, 1359.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1390.3ms\n",
            "Speed: 0.0ms preprocess, 1390.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1530.9ms\n",
            "Speed: 0.0ms preprocess, 1530.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1359.1ms\n",
            "Speed: 15.6ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1359.1ms\n",
            "Speed: 0.0ms preprocess, 1359.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1405.9ms\n",
            "Speed: 0.0ms preprocess, 1405.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1374.7ms\n",
            "Speed: 0.0ms preprocess, 1374.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ],
      "source": [
        "max_frames = 30\n",
        "# Define horizontal line positions\n",
        "entry_line_y = 100\n",
        "exit_line_y = 600\n",
        "\n",
        "# Counters for entries and exits\n",
        "entry_count = 0\n",
        "exit_count = 0\n",
        "\n",
        "# Add the following inside your while loop\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Draw horizontal lines\n",
        "    cv2.line(frame, (0, entry_line_y), (frame.shape[1], entry_line_y), (0, 255, 0), 2)  # Entry line\n",
        "    cv2.line(frame, (0, exit_line_y), (frame.shape[1], exit_line_y), (0, 0, 255), 2)    # Exit line\n",
        "\n",
        "    # Run YOLO tracking on the frame\n",
        "    results = model.track(frame, persist=True, classes=[0])\n",
        "\n",
        "    if results[0].boxes.data is not None:\n",
        "        boxes = results[0].boxes.xyxy.cpu()\n",
        "        if results[0].boxes.id is not None:\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "        else:\n",
        "            track_ids = []\n",
        "        class_indices = results[0].boxes.cls.int().cpu().tolist()\n",
        "        confidences = results[0].boxes.conf.cpu()\n",
        "\n",
        "        for box, track_id, class_idx, conf in zip(boxes, track_ids, class_indices, confidences):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            cx = (x1 + x2) // 2\n",
        "            cy = (y1 + y2) // 2\n",
        "            class_name = class_names[class_idx]\n",
        "\n",
        "            # Update track history\n",
        "            if track_id not in track_history:\n",
        "                track_history[track_id] = []\n",
        "            track_history[track_id].append((cx, cy))\n",
        "\n",
        "            if len(track_history[track_id]) > max_frames:\n",
        "                track_history[track_id].pop(0)\n",
        "\n",
        "            # Check crossing entry and exit lines\n",
        "            if len(track_history[track_id]) > 1:\n",
        "                prev_cy = track_history[track_id][-2][1]\n",
        "\n",
        "                # Check entry\n",
        "                if prev_cy < entry_line_y <= cy:\n",
        "                    entry_count += 1\n",
        "                    print(f\"Entry detected: ID {track_id}\")\n",
        "\n",
        "                # Check exit\n",
        "                if prev_cy > exit_line_y >= cy:\n",
        "                    exit_count += 1\n",
        "                    print(f\"Exit detected: ID {track_id}\")\n",
        "\n",
        "            # Draw center point and track\n",
        "            cv2.circle(frame, (cx, cy), 4, (0, 0, 255), -1)\n",
        "            cv2.putText(frame, f\"ID: {track_id} {class_name}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Draw track history\n",
        "            if len(track_history[track_id]) > 1:\n",
        "                for i in range(1, len(track_history[track_id])):\n",
        "                    cv2.line(frame, track_history[track_id][i-1], track_history[track_id][i], (255, 0, 0), 2)\n",
        "\n",
        "    # Display entry/exit counts\n",
        "    cv2.putText(frame, f\"Entries: {entry_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "    cv2.putText(frame, f\"Exits: {exit_count}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "    cv2.imshow('yolo_tracking', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}